{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL-ver+content(fc128+fc2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e53c7aa26aca40e3874c5559abaeb51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d891a0d47a942399480b84538b3784e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72a601426c8a474daee7a82615e4e9e7",
              "IPY_MODEL_7c93f8a1142b401090a0607aafec5994"
            ]
          }
        },
        "5d891a0d47a942399480b84538b3784e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72a601426c8a474daee7a82615e4e9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cb0e566f94a84b9098abd5ba1e9f31b4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a1224a4cab84002a12d97686d002118"
          }
        },
        "7c93f8a1142b401090a0607aafec5994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bec619edd6b64b73847711f5be4a4b8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.31kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eda6395f05a24e0baa79ced38d939866"
          }
        },
        "cb0e566f94a84b9098abd5ba1e9f31b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a1224a4cab84002a12d97686d002118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bec619edd6b64b73847711f5be4a4b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eda6395f05a24e0baa79ced38d939866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0e8519ae73d4f77b777d9ec9cc8bda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4b8f4932c074dd398c34de1a129c7b7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6d06ca2b9fc4c86a565697486ebeb63",
              "IPY_MODEL_00f69bae1f344f31bc2a74e1c5299c7c"
            ]
          }
        },
        "b4b8f4932c074dd398c34de1a129c7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6d06ca2b9fc4c86a565697486ebeb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efae242b43ff459c910c7d13eefc47f0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02a34f1d9d9d4576855378f4b1f7534f"
          }
        },
        "00f69bae1f344f31bc2a74e1c5299c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f6cafc7ec93443fa6a53789c44ec1d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:41&lt;00:00, 10.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2f14d1b0a8a467d98af6bfa40d7454e"
          }
        },
        "efae242b43ff459c910c7d13eefc47f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02a34f1d9d9d4576855378f4b1f7534f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f6cafc7ec93443fa6a53789c44ec1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2f14d1b0a8a467d98af6bfa40d7454e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "78CGu-xCuAFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "7418fd43-70d8-4885-8f8a-7f9204a5ffbe"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2DHMoWuYW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "1fc9a6e3-9560-4c8b-800c-b3703e64142b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=27d7e8e1bc7d2e1e58d4af5f278d686fa273d7bfa7398de57a58474d1919232d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogzkR4cuudr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "import numpy\n",
        "import os\n",
        "import codecs\n",
        "import random\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfyhQk8DueQc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8fcaf088-b446-44aa-aba5-6db6e90a3e1a"
      },
      "source": [
        "# # If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "\t# Tell PyTorch to use the GPU.    \n",
        "\tdevice = torch.device(\"cuda\")\n",
        "\n",
        "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "\tprint('No GPU available, using the CPU instead.')\n",
        "\tdevice = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwHdqsF3ujUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def _label_node_index(node, n=0):\n",
        "    node['index'] = n\n",
        "    for child in node['c']:\n",
        "        n += 1\n",
        "        _label_node_index(child, n)\n",
        "\n",
        "\n",
        "def _gather_node_attributes(node, key):\n",
        "    features = [node[key]]\n",
        "    for child in node['c']:\n",
        "        features.extend(_gather_node_attributes(child, key))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _gather_adjacency_list(node):\n",
        "    adjacency_list = []\n",
        "    for child in node['c']:\n",
        "        adjacency_list.append([node['index'], child['index']])\n",
        "        adjacency_list.extend(_gather_adjacency_list(child))\n",
        "\n",
        "    return adjacency_list\n",
        "\n",
        "\n",
        "def convert_tree_to_tensors(tree, tweet_id, device=device):\n",
        "    # Label each node with its walk order to match nodes to feature tensor indexes\n",
        "    # This modifies the original tree as a side effect\n",
        "    _label_node_index(tree)\n",
        "\n",
        "    features = _gather_node_attributes(tree, 'f')\n",
        "    attention = _gather_node_attributes(tree, 'a')\n",
        "    old_features = _gather_node_attributes(tree, 'k')\n",
        "    labels = _gather_node_attributes(tree, 'l')\n",
        "    tweetid = tweet_id\n",
        "    root_label = [labels[0]]\n",
        "    s_gt = [tree['class_gt']]\n",
        "    adjacency_list = _gather_adjacency_list(tree)\n",
        "\n",
        "    node_order, edge_order = calculate_evaluation_orders(adjacency_list, len(features))\n",
        "\n",
        "    root_node = [0]\n",
        "\n",
        "    return {\n",
        "        'f': torch.tensor(features, dtype=torch.long),\n",
        "        'a':torch.tensor(attention,  dtype=torch.float32),\n",
        "        'k':torch.tensor(old_features, dtype=torch.float32),\n",
        "        's_gt':torch.tensor(s_gt,dtype=torch.long),\n",
        "        'l': torch.tensor(labels,  dtype=torch.float32),\n",
        "        'root_l': torch.tensor(root_label, dtype=torch.float32),\n",
        "        'root_n': torch.tensor(root_node,  dtype=torch.int64),\n",
        "        'node_order': torch.tensor(node_order,  dtype=torch.int64),\n",
        "        'adjacency_list': torch.tensor(adjacency_list,  dtype=torch.int64),\n",
        "        'edge_order': torch.tensor(edge_order,  dtype=torch.int64),\n",
        "        'tweet_id' : torch.tensor(tweet_id, dtype=torch.int64)\n",
        "    }\n",
        "\n",
        "def calculate_evaluation_orders(adjacency_list, tree_size):\n",
        "    '''Calculates the node_order and edge_order from a tree adjacency_list and the tree_size.\n",
        "\n",
        "    The TreeLSTM model requires node_order and edge_order to be passed into the model along\n",
        "    with the node features and adjacency_list.  We pre-calculate these orders as a speed\n",
        "    optimization.\n",
        "    '''\n",
        "    adjacency_list = numpy.array(adjacency_list)\n",
        "\n",
        "    node_ids = numpy.arange(tree_size, dtype=int)\n",
        "\n",
        "    node_order = numpy.zeros(tree_size, dtype=int)\n",
        "    unevaluated_nodes = numpy.ones(tree_size, dtype=bool)\n",
        "    # print(adjacency_list)\n",
        "    if(len(adjacency_list)==0):\n",
        "        return [0],[]\n",
        "    parent_nodes = adjacency_list[:, 0]\n",
        "    child_nodes = adjacency_list[:, 1]\n",
        "\n",
        "    n = 0\n",
        "    while unevaluated_nodes.any():\n",
        "        # Find which child nodes have not been evaluated\n",
        "        unevaluated_mask = unevaluated_nodes[child_nodes]\n",
        "\n",
        "        # Find the parent nodes of unevaluated children\n",
        "        unready_parents = parent_nodes[unevaluated_mask]\n",
        "\n",
        "        # Mark nodes that have not yet been evaluated\n",
        "        # and which are not in the list of parents with unevaluated child nodes\n",
        "        nodes_to_evaluate = unevaluated_nodes & ~numpy.isin(node_ids, unready_parents)\n",
        "\n",
        "        node_order[nodes_to_evaluate] = n\n",
        "        unevaluated_nodes[nodes_to_evaluate] = False\n",
        "\n",
        "        n += 1\n",
        "\n",
        "    edge_order = node_order[parent_nodes]\n",
        "\n",
        "    return node_order, edge_order\n",
        "\n",
        "\n",
        "def batch_tree_input(batch):\n",
        "    '''Combines a batch of tree dictionaries into a single batched dictionary for use by the TreeLSTM model.\n",
        "\n",
        "    batch - list of dicts with keys ('f', 'node_order', 'edge_order', 'adjacency_list')\n",
        "    returns a dict with keys ('f', 'node_order', 'edge_order', 'adjacency_list', 'tree_sizes')\n",
        "    '''\n",
        "    tree_sizes = [b['f'].shape[0] for b in batch]\n",
        "\n",
        "    batched_features = torch.cat([b['f'] for b in batch])\n",
        "    batched_attentions = torch.cat([b['a'] for b in batch])\n",
        "    batched_old_features = torch.cat([b['k'] for b in batch])\n",
        "    batched_node_order = torch.cat([b['node_order'] for b in batch])\n",
        "\n",
        "    idx = 0\n",
        "    root_li = []\n",
        "\n",
        "    for b in batch:\n",
        "        root_li.append(idx)\n",
        "        idx += len(b['node_order'])\n",
        "\n",
        "    batched_root = torch.tensor(root_li,dtype=torch.int64)\n",
        "\n",
        "    batched_edge_order = torch.cat([b['edge_order'] for b in batch])\n",
        "\n",
        "    batched_labels = torch.cat([b['l'] for b in batch])\n",
        "\n",
        "    batched_root_labels = torch.cat([b['root_l'] for b in batch])\n",
        "    batched_cont_labels = torch.cat([b['s_gt'] for b in batch])\n",
        "    batched_adjacency_list = []\n",
        "    offset = 0\n",
        "    for n, b in zip(tree_sizes, batch):\n",
        "        batched_adjacency_list.append(b['adjacency_list'] + offset)\n",
        "        offset += n\n",
        "    batched_adjacency_list = torch.cat(batched_adjacency_list)\n",
        "\n",
        "    return {\n",
        "        'f': batched_features,\n",
        "        'a': batched_attentions,\n",
        "        'k': batched_old_features,\n",
        "        's_gt':batched_cont_labels,\n",
        "        'node_order': batched_node_order,\n",
        "        'edge_order': batched_edge_order,\n",
        "        'adjacency_list': batched_adjacency_list,\n",
        "        'tree_sizes': tree_sizes,\n",
        "        'root_node': batched_root,\n",
        "        'root_label': batched_root_labels,\n",
        "        'l': batched_labels\n",
        "    }\n",
        "\n",
        "\n",
        "def unbatch_tree_tensor(tensor, tree_sizes):\n",
        "    '''Convenience functo to unbatch a batched tree tensor into individual tensors given an array of tree_sizes.\n",
        "\n",
        "    sum(tree_sizes) must equal the size of tensor's zeroth dimension.\n",
        "    '''\n",
        "    return torch.split(tensor, tree_sizes, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbXOaim1unNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeLSTM(torch.nn.Module):\n",
        "\t'''PyTorch TreeLSTM model that implements efficient batching.\n",
        "\t'''\n",
        "\tdef __init__(self,model_name,trainable_layers, in_features, out_features,mode):\n",
        "\t\t'''TreeLSTM class initializer\n",
        "\n",
        "\t\tTakes in int sizes of in_features and out_features and sets up model Linear network layers.\n",
        "\t\t'''\n",
        "\t\tsuper().__init__()\n",
        "\t\tprint(\"model intialising...\")\n",
        "\t\tself.in_features = in_features\n",
        "\t\tself.out_features = out_features\n",
        "\t\tself.mode = mode\n",
        "\n",
        "\t\tself.model_name = model_name\n",
        "\n",
        "\t\tif model_name == 'BERT':\n",
        "\t\t\tself.BERT_model  = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\t\telif model_name == 'ROBERTA':\n",
        "\t\t\tself.BERT_model  = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\t\telif model_name == 'XLNET':\n",
        "\t\t\tself.BERT_model = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
        "\t\telif model_name == 'T5':\n",
        "\t\t\tself.BERT_model = T5Model.from_pretrained(\"t5-base\")\n",
        "   \n",
        "\t\tfor name,param in self.BERT_model.named_parameters():\n",
        "\t\t\tflag = False\n",
        "\t\t\tfor num in trainable_layers:\n",
        "\t\t\t\tif 'layer.'+str(num)+'.' in name:\n",
        "\t\t\t\t\tparam.requires_grad = True\n",
        "\t\t\t\t\tflag = True\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\tif not flag:\n",
        "\t\t\t\tif 'pooler' in name or 'embedding' in name:\n",
        "\t\t\t\t\tparam.requires_grad = True\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tparam.requires_grad = False\n",
        "\n",
        "\t\tself.W_iou = torch.nn.Linear(self.in_features, 3 * self.out_features)\n",
        "\t\tself.U_iou = torch.nn.Linear(self.out_features, 3 * self.out_features, bias=False)\n",
        "\n",
        "\t\tself.W_f = torch.nn.Linear(self.in_features, self.out_features)\n",
        "\t\tself.U_f = torch.nn.Linear(self.out_features, self.out_features, bias=False)\n",
        "\t\tself.fc = torch.nn.Linear(self.out_features,2)\n",
        "  \n",
        "\t\tself.cont_fc1 = torch.nn.Linear(self.in_features,self.out_features)\n",
        "\t\tself.cont_fc2 = torch.nn.Linear(self.out_features,4) \n",
        "\t\n",
        "\tdef forward(self, features,attentions,old_features,node_order, adjacency_list, edge_order, root_node, root_label):\n",
        "\t\t'''Run TreeLSTM model on a tree data structure with node features\n",
        "\n",
        "\t\tTakes Tensors encoding node features, a tree node adjacency_list, and the order in which \n",
        "\t\tthe tree processing should proceed in node_order and edge_order.\n",
        "\t\t'''\n",
        "\n",
        "\t\t# Total number of nodes in every tree in the batch\n",
        "\t\tbatch_size = node_order.shape[0]\n",
        "\n",
        "\t\t# Retrive device the model is currently loaded on to generate h, c, and h_sum result buffers\n",
        "\t\tdevice = next(self.parameters()).device\n",
        "\n",
        "\t\t# h and c states for every node in the batch\n",
        "\t\t# h - hidden state\n",
        "\t\t# c - memory state\n",
        "\t\th = torch.zeros(batch_size, self.out_features, device=device)\n",
        "\t\t\n",
        "\t\tc = torch.zeros(batch_size, self.out_features, device=device)\n",
        "\n",
        "\t\thidden_states,_ = self.BERT_model(input_ids=features,attention_mask=attentions)  \n",
        "\n",
        "\t\tif self.mode==\"cls\":\n",
        "\t\t\toutput_vectors = hidden_states[:,0]\n",
        "\t\tif self.mode==\"avg\":\n",
        "\t\t\tinput_mask_expanded = attentions.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "\t\t\tsum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
        "\t\t\tsum_mask = input_mask_expanded.sum(1)\n",
        "\t\t\toutput_vectors= sum_embeddings / sum_mask\n",
        "\t\t\t\n",
        "\t\toutput_vectors = torch.cat([output_vectors,old_features],axis=1)\n",
        "  \n",
        "\t\tx = output_vectors[root_node,:]\n",
        "\t\tx = self.cont_fc1(x)\n",
        "\t\tx = self.cont_fc2(x)\n",
        "\t\t# x = torch.nn.functional.softmax(x,dim=1)\n",
        "\n",
        "\t\tfor n in range(node_order.max() + 1):\n",
        "\t\t\tself._run_lstm(n, h, c, output_vectors, node_order, adjacency_list, edge_order)\n",
        "  \n",
        "\t\th_root = h[root_node, :]\n",
        "\t\t# h_root = torch.nn.functional.softmax(h_root, dim = 1)\n",
        "\t\tout = self.fc(h_root)\n",
        "\t\t# out = torch.nn.functional.softmax(out, dim = 1)\n",
        "  \n",
        "\t\treturn h, out, c, x\n",
        "\n",
        "\tdef _run_lstm(self, iteration, h, c, features, node_order, adjacency_list, edge_order):\n",
        "\t\t'''Helper function to evaluate all tree nodes currently able to be evaluated.\n",
        "\t\t'''\n",
        "\t\tnode_mask = node_order == iteration\n",
        "\n",
        "\t\t# edge_mask is a tensor of size E x 1\n",
        "\t\tedge_mask = edge_order == iteration\n",
        "\n",
        "\t\tx = features[node_mask, :]\n",
        "\t\tif iteration == 0:\n",
        "\t\t\tiou = self.W_iou(x)\n",
        "\t\telse:\n",
        "\t\t\t# adjacency_list is a tensor of size e x 2\n",
        "\t\t\tadjacency_list = adjacency_list[edge_mask, :]\n",
        "\n",
        "\t\t\tparent_indexes = adjacency_list[:, 0]\n",
        "\t\t\tchild_indexes = adjacency_list[:, 1]\n",
        "\n",
        "\t\t\t# child_h and child_c are tensors of size e x 1\n",
        "\t\t\tchild_h = h[child_indexes, :]\n",
        "\t\t\tchild_c = c[child_indexes, :]\n",
        "\n",
        "\t\t\t# Add child hidden states to parent offset locations\n",
        "\t\t\t_, child_counts = torch.unique_consecutive(parent_indexes, return_counts=True)\n",
        "\t\t\tchild_counts = tuple(child_counts)\n",
        "\t\t\tparent_children = torch.split(child_h, child_counts)\n",
        "\t\t\tparent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "\t\t\th_sum = torch.stack(parent_list)\n",
        "\t\t\tiou = self.W_iou(x) + self.U_iou(h_sum)\n",
        "\n",
        "\n",
        "\t\t# i, o and u are tensors of size n x M\n",
        "\t\ti, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n",
        "\t\ti = torch.sigmoid(i)\n",
        "\t\to = torch.sigmoid(o)\n",
        "\t\tu = torch.tanh(u)\n",
        "\n",
        "\t\tif iteration == 0:\n",
        "\t\t\tc[node_mask, :] = i * u\n",
        "\t\telse:\n",
        "\t\t\t# f is a tensor of size e x M\n",
        "\t\t\tf = self.W_f(features[parent_indexes, :]) + self.U_f(child_h)\n",
        "\t\t\tf = torch.sigmoid(f)\n",
        "\t\t\t# fc is a tensor of size e x M\n",
        "\t\t\tfc = f * child_c\n",
        "\n",
        "\t\t\t# Add the calculated f values to the parent's memory cell state\n",
        "\t\t\tparent_children = torch.split(fc, child_counts)\n",
        "\t\t\tparent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "\t\t\tc_sum = torch.stack(parent_list)\n",
        "\t\t\tc[node_mask, :] = i * u + c_sum\n",
        "\n",
        "\t\th[node_mask, :] = o * torch.tanh(c[node_mask])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBc_Z_6Qun7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, name, val_acc=0):\n",
        "  state = {\n",
        "      'model':model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'val_acc': val_acc\n",
        "  }\n",
        "  torch.save(state, name)\n",
        "\n",
        "def load_model(model, name):\n",
        "  state = torch.load(name)\n",
        "  model.load_state_dict(state['model'])\n",
        "  optimizer.load_state_dict(state['optimizer'])\n",
        "  print('Validation accuracy of the model is ', state.get('val_acc'))\n",
        "  return state.get('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm-ijT60uqld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_val = 40\n",
        "random.seed(seed_val)\n",
        "numpy.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "path = \"./drive/My Drive/\"\n",
        "IN_FEATURES = 808\n",
        "OUT_FEATURES = 128\n",
        "NUM_ITERATIONS = 10\n",
        "BATCH_SIZE = 16\n",
        "TRAINABLE_LAYERS = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "learning_rates = [2e-5,1e-5]\n",
        "name = path + \"mtl_ver+cont.pt\"\n",
        "name2 = path + \"mtl_ver+cont_2.pt\"\n",
        "mode = 'cls'\n",
        "MODEL_NAME = 'BERT'\n",
        "if MODEL_NAME == 'BERT':\n",
        "    tree_path = './drive/My Drive/PT_PHEME5_FeatBERT40_Depth5_maxR5_MTL/'\n",
        "elif MODEL_NAME == 'ROBERTA':\n",
        "    tree_path = './drive/My Drive/Parsed-Trees-Pad32_FeatROBERT40_Depth5_maxR5/'\n",
        "elif MODEL_NAME == 'XLNET':\n",
        "    tree_path = './drive/My Drive/Parsed-Trees-Pad32_FeatXLNET40_Depth5_maxR5/'\n",
        "elif MODEL_NAME =='T5':\n",
        "    tree_path = './drive/My Drive/Parsed-Trees-Pad32_FeatT540_Depth5_maxR5/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_JqdJUOu1aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing(test_trees,model,epoch):\n",
        "    print('Now Testing:', test_file)\n",
        "    train_acc = 0\n",
        "    total = 0\n",
        "    predicted = []\n",
        "    ground = []\n",
        "    cont_pred = []\n",
        "    cont_ground = []\n",
        "    model.eval()\n",
        "    prob = []\n",
        "    pred =[]\n",
        "    tweetid = []\n",
        "    for test in test_trees:\n",
        "        try:\n",
        "            h_test,h_test_root,c,cont_out = model(\n",
        "                    test['f'].to(device),\n",
        "                    test['a'].to(device),\n",
        "                    test['k'].to(device),\n",
        "                    test['node_order'].to(device),\n",
        "                    test['adjacency_list'].to(device),\n",
        "                    test['edge_order'].to(device),\n",
        "                    test['root_n'].to(device),\n",
        "                    test['root_l'].to(device)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        true_label_vals = test['root_l'].to('cpu')\n",
        "        pred_label_vals = h_test_root.detach().cpu()\n",
        "        pred_v, pred_label = torch.max(pred_label_vals, 1)\n",
        "        true_label = true_label_vals[0][1]\n",
        "        predicted.append(pred_label)\n",
        "        ground.append(true_label)\n",
        "        \n",
        "        cont_true_val = test['s_gt']\n",
        "        cont_pred_vals = cont_out.detach().cpu()\n",
        "        cont_v,cont_label = torch.max(cont_pred_vals, 1)\n",
        "        cont_pred.append(cont_label)\n",
        "        cont_ground.append(cont_true_val)\n",
        "    print(\"=================Testing===================\")\n",
        "    print(test_file)\n",
        "    print(\"Verification:\")\n",
        "    print(classification_report(ground,predicted,digits=5))\n",
        "    print(\"Content Classification\")\n",
        "    print(classification_report(cont_ground,cont_pred,digits=5))\n",
        "\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4sHC9Uwuqsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,tree_batch,mode=\"train\"):\n",
        "    err_count = 0\n",
        "    loss = 0\n",
        "    pred_label = []\n",
        "    g_labels = []\n",
        "    cont_pred_labels = []\n",
        "    cont_gt = []\n",
        "\n",
        "\n",
        "    h,h_root,c, cont_out = model(\n",
        "        tree_batch['f'].to(device),\n",
        "        tree_batch['a'].to(device),\n",
        "        tree_batch['k'].to(device),\n",
        "        tree_batch['node_order'].to(device),\n",
        "        tree_batch['adjacency_list'].to(device),\n",
        "        tree_batch['edge_order'].to(device),\n",
        "        tree_batch['root_node'].to(device),\n",
        "        tree_batch['root_label'].to(device)\n",
        "    )\n",
        "    root_labels = tree_batch['root_label'].to(device)\n",
        "    pred_label_vals = h_root.detach().cpu()\n",
        "    pred_v, pred_label = torch.max(pred_label_vals, 1)\n",
        "    root = root_labels.to('cpu')\n",
        "    g_labels = [t[1] for t in root]\n",
        "\n",
        "    cont_labels = tree_batch['s_gt'].to(device)\n",
        "    cont_label_vals = cont_out.detach().cpu()\n",
        "    cont_v,cont_label = torch.max(cont_label_vals, 1)\n",
        "    cont_gt = tree_batch['s_gt'] \n",
        "\n",
        "    loss = loss_function(h_root,cont_out,root_labels,cont_labels,0.5,0.5,False)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    if mode==\"train\":\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "    return loss,pred_label,g_labels,cont_label,cont_gt,err_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4u017t9unqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files = ['charliehebdo.txt', 'ottawashooting.txt','germanwings-crash.txt','sydneysiege.txt'] \n",
        "# cont_tweets = {}\n",
        "# for f in files:\n",
        "#     f = codecs.open(path+\"situational_tweets/\"+f[:-4]+\"_FOUR_CLEAN_ANNOTATE_110520.txt\")\n",
        "#     for line in f:\n",
        "#         line = line.split(\"\\t\")\n",
        "#         cont_tweets[int(line[1])] = int(line[8])-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXmSDnzdcIVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(trees,frac):\n",
        "    data = [[] for _ in range(4)]\n",
        "    for tree in trees:\n",
        "        data[int(tree['s_gt'].item())].append(tree)\n",
        "    # length = [int(frac*len(data[i])) for i in range(4)]\n",
        "    # val_li = []\n",
        "    # val_li = [val_li+data[i][:length[i]] for i in range(4)]\n",
        "    # random.shuffle(val_li)\n",
        "    # train_li = []\n",
        "    # train_li = [train_li+data[i][length[i]:] for i in range(4)]\n",
        "    # random.shuffle(train_li)\n",
        "    # return train_li[0],val_li[0]\n",
        "    length = [int(frac*len(data[i])) for i in range(4)]\n",
        "    print([len(data[i][:length[i]]) for i in range(4)])\n",
        "    val_li = []\n",
        "    for i in range(4):\n",
        "        val_li.extend(data[i][:length[i]])\n",
        "    random.shuffle(val_li)\n",
        "\n",
        "    train_li = []\n",
        "    for i in range(4):\n",
        "        train_li.extend(data[i][length[i]:])\n",
        "    random.shuffle(train_li)\n",
        "    return train_li,val_li"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6R6PLKuwHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "1a6eafc8-f838-428b-a3a8-34affe7dd9a1"
      },
      "source": [
        "files = ['charliehebdo.txt', 'ottawashooting.txt','germanwings-crash.txt','sydneysiege.txt'] \n",
        "\n",
        "tree_li = {}\n",
        "val_li = {}\n",
        "y = []\n",
        "for filename in files:\n",
        "    err = 0\n",
        "    count = 0\n",
        "    input_file = codecs.open(tree_path + filename, 'r', 'utf-8')\n",
        "    tree_li[filename]=[]\n",
        "    for row in input_file:\n",
        "        s = row.strip().split('\\t')\n",
        "        count += 1\n",
        "        tweet_id = int(s[0])\n",
        "        curr_tree = eval(s[1])\n",
        "        try:\n",
        "            # ,cont_tweets[tweet_id],\n",
        "            curr_tensor = convert_tree_to_tensors(curr_tree,tweet_id)\n",
        "        except Exception as e:\n",
        "            err += 1\n",
        "            print(e)\n",
        "            continue\n",
        "        y.append(int(curr_tree['class_gt']))\n",
        "        tree_li[filename].append(curr_tensor)\n",
        "    random.shuffle(tree_li[filename])\n",
        "    tree_li[filename],val_li[filename] = split_data(tree_li[filename],0.1)\n",
        "    input_file.close()\n",
        "    print(\"errors \", err)\n",
        "    print(\"file count \",count)\n",
        "    print(\"filename\",len(tree_li[filename]),len(val_li[filename]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20, 37, 17, 132]\n",
            "errors  0\n",
            "file count  2079\n",
            "filename 1873 206\n",
            "[18, 18, 17, 34]\n",
            "errors  0\n",
            "file count  890\n",
            "filename 803 87\n",
            "[7, 13, 11, 14]\n",
            "errors  0\n",
            "file count  469\n",
            "filename 424 45\n",
            "[15, 23, 22, 61]\n",
            "errors  0\n",
            "file count  1221\n",
            "filename 1100 121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIU6fl7xFkgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5452ee70-c9be-45a7-9f55-65a3659c14cd"
      },
      "source": [
        "# tree_li['charliehebdo.txt'][0]['s_gt'].item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THwGI2kspvpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "content_weight_dict = {}\n",
        "for test_file in files:\n",
        "    train_y = []\n",
        "    for f in files:\n",
        "        if f != test_file:\n",
        "            for t in tree_li[f]:\n",
        "                train_y.append(t['s_gt'][0].tolist())\n",
        "    content_weight_dict[test_file] = torch.tensor(compute_class_weight('balanced',numpy.unique(train_y),train_y),device=device,dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ESSGR20UJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "verification_weight_dict = {}\n",
        "for test_file in files:\n",
        "    y = []\n",
        "    for filename in files:\n",
        "        if filename != test_file:\n",
        "            for tree in tree_li[filename]:\n",
        "                y.append(int(tree['root_l'].tolist()[0][1]))\n",
        "    verification_weight_dict[test_file] = torch.tensor(compute_class_weight('balanced',numpy.unique(y),y)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFrcEljt6LOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b119572f-27d4-4aab-edf7-5ff5663f85f5"
      },
      "source": [
        "content_weight_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'charliehebdo.txt': tensor([1.5350, 1.1705, 1.2538, 0.5894], device='cuda:0'),\n",
              " 'germanwings-crash.txt': tensor([1.8956, 1.3296, 1.8259, 0.4603], device='cuda:0'),\n",
              " 'ottawashooting.txt': tensor([2.1392, 1.2675, 1.8422, 0.4544], device='cuda:0'),\n",
              " 'sydneysiege.txt': tensor([1.8023, 1.2400, 1.8675, 0.4755], device='cuda:0')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZryIaKZ6Ogi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5e3a8d6c-0c78-4896-e2a8-8762fefb5b85"
      },
      "source": [
        "verification_weight_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'charliehebdo.txt': tensor([0.9576, 1.0463], device='cuda:0', dtype=torch.float64),\n",
              " 'germanwings-crash.txt': tensor([0.7628, 1.4512], device='cuda:0', dtype=torch.float64),\n",
              " 'ottawashooting.txt': tensor([0.7394, 1.5441], device='cuda:0', dtype=torch.float64),\n",
              " 'sydneysiege.txt': tensor([0.7550, 1.4804], device='cuda:0', dtype=torch.float64)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew-ho99yyQe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(logits1,logits2,glabel1,glabel2,lambda1,lambda2,flood):\n",
        "\tloss1 = loss_function1(logits1,glabel1)\n",
        "\tloss2 = loss_function2(logits2,glabel2)\n",
        "\tloss = lambda1*loss1 + lambda2*loss2\n",
        "\tif flood:\n",
        "\t\tloss = (loss-0.25).abs()+0.25\n",
        "\treturn loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ7UGdmRuwLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e53c7aa26aca40e3874c5559abaeb51d",
            "5d891a0d47a942399480b84538b3784e",
            "72a601426c8a474daee7a82615e4e9e7",
            "7c93f8a1142b401090a0607aafec5994",
            "cb0e566f94a84b9098abd5ba1e9f31b4",
            "0a1224a4cab84002a12d97686d002118",
            "bec619edd6b64b73847711f5be4a4b8a",
            "eda6395f05a24e0baa79ced38d939866",
            "c0e8519ae73d4f77b777d9ec9cc8bda1",
            "b4b8f4932c074dd398c34de1a129c7b7",
            "b6d06ca2b9fc4c86a565697486ebeb63",
            "00f69bae1f344f31bc2a74e1c5299c7c",
            "efae242b43ff459c910c7d13eefc47f0",
            "02a34f1d9d9d4576855378f4b1f7534f",
            "7f6cafc7ec93443fa6a53789c44ec1d1",
            "a2f14d1b0a8a467d98af6bfa40d7454e"
          ]
        },
        "outputId": "fe862b8f-e111-45d9-8265-46bc470c5fe5"
      },
      "source": [
        "for learning_rate in learning_rates:\n",
        "    print(\"learning rate = \",learning_rate)\n",
        "    for test in files:\n",
        "        model = TreeLSTM(MODEL_NAME,TRAINABLE_LAYERS, IN_FEATURES, OUT_FEATURES,mode=mode).train()\n",
        "        model.cuda()\n",
        "        optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay = 0.01)\n",
        "        # optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,amsgrad=True)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',patience=2, verbose=True)\n",
        "        loss_function1 = torch.nn.BCEWithLogitsLoss(weight = verification_weight_dict[test])\n",
        "        loss_function2 = torch.nn.CrossEntropyLoss(weight = content_weight_dict[test])\n",
        "        test_file = test\n",
        "        model.train()\n",
        "        print('Training Set:', set(files) - {test_file})\n",
        "        test_trees = []\n",
        "        train_trees = []\n",
        "        val_data = []\n",
        "        for filename in files:\n",
        "            if filename == test:\n",
        "                test_trees.extend(tree_li[filename])\n",
        "                test_trees.extend(val_li[filename])\n",
        "            else:\n",
        "                curr_tree_dataset = TreeDataset(tree_li[filename])\n",
        "                train_trees.extend(curr_tree_dataset)\n",
        "                val_data.extend(TreeDataset(val_li[filename]))\n",
        "        print(len(test_trees))\n",
        "        print(\"size of training data\",len(train_trees))\n",
        "        print(\"training started....\")\n",
        "        prev_loss = 1\n",
        "        prev_acc = 0\n",
        "        for i in range(NUM_ITERATIONS):\n",
        "            model.train()\n",
        "            model.zero_grad() \n",
        "            total_loss = 0\n",
        "            data_gen = []\n",
        "            length = []\n",
        "\n",
        "            train_gen = DataLoader(\n",
        "                    train_trees,\n",
        "                    collate_fn=batch_tree_input,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    shuffle = True\n",
        "                )\n",
        "            val_gen = DataLoader(val_data,\n",
        "                    collate_fn=batch_tree_input,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    shuffle = True)\n",
        "            j = 0\n",
        "            avg_loss=0\n",
        "            ground_labels = []\n",
        "            predicted_labels = []\n",
        "            cont_ground_labels = []\n",
        "            cont_predicted_labels = []\n",
        "            val_ground_labels = []\n",
        "            val_predicted_labels= []\n",
        "            val_cont_ground = []\n",
        "            val_cont_predicted = []\n",
        "            err_count = 0\n",
        "            batch_no = 0\n",
        "            for tree_batch in train_gen:\n",
        "                loss,p_labels1,g_labels1,p_labels2,g_labels2,err = train(model,tree_batch,\"train\")\n",
        "                err_count+=err\n",
        "                if err!=1:\n",
        "                    ground_labels.extend(g_labels1)\n",
        "                    predicted_labels.extend(p_labels1)\n",
        "                    cont_ground_labels.extend(g_labels2)\n",
        "                    cont_predicted_labels.extend(p_labels2)\n",
        "                    j = j+1\n",
        "                    avg_loss += loss\n",
        "                    total_loss += loss\n",
        "                batch_no += 1\n",
        "            print(\"validation started..\",len(val_data))\n",
        "            model.eval()\n",
        "            val_avg_loss = 0\n",
        "            val_j = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_gen:\n",
        "                    loss,p_labels1,g_labels1,p_labels2,g_labels2,err = train(model,batch,\"eval\")\n",
        "                    err_count+=err\n",
        "                    if err!=1:\n",
        "                        val_ground_labels.extend(g_labels1)\n",
        "                        val_predicted_labels.extend(p_labels1)\n",
        "                        val_cont_ground.extend(g_labels2)\n",
        "                        val_cont_predicted.extend(p_labels2)\n",
        "                        val_j += 1\n",
        "                        val_avg_loss += loss\n",
        "            val_acc1 = accuracy_score(val_ground_labels,val_predicted_labels)\n",
        "            val_acc2 = accuracy_score(val_cont_ground,val_cont_predicted)\n",
        "            val_acc = val_acc1*0.5 + val_acc2*0.5\n",
        "            scheduler.step(val_acc)\n",
        "            if(prev_acc<=val_acc):\n",
        "                save_model(model, name , val_acc)\n",
        "                prev_acc = val_acc\n",
        "            print(\"errors \",err_count)\n",
        "            print('Iteration ', i)\n",
        "            print('Training Loss: ',avg_loss/j)\t\n",
        "            print('Validation loss: ',val_avg_loss/val_j)\n",
        "\n",
        "            print('verification training accuracy: ',accuracy_score(ground_labels,predicted_labels))\n",
        "            print('verification validation accuracy: ',val_acc1)\n",
        "            print('verification training confusion matrix: ',confusion_matrix(ground_labels, predicted_labels))\n",
        "            print('Content classification training accuracy: ',accuracy_score(cont_ground_labels,cont_predicted_labels))\n",
        "            print('Content classification validation accuracy: ',val_acc2)\n",
        "            print('Content classification training confusion matrix: ',confusion_matrix(cont_ground_labels, cont_predicted_labels))\n",
        "\n",
        "            ###############################################################################################################\n",
        "            ##############################...................TESTING......................#################################\n",
        "            ###############################################################################################################\n",
        "            if (i+1)%5==0:\n",
        "                with torch.no_grad():\n",
        "                    save_model(model,name2,val_acc)\n",
        "                    # test_model = TreeLSTM(TRAINABLE_LAYERS, IN_FEATURES, OUT_FEATURES,\"cls\").eval()\n",
        "                    # test_model.cuda()\n",
        "                    output = load_model(model,name)\n",
        "                    ret_val = testing(test_trees,model,i+1)\n",
        "                output = load_model(model,name2)\n",
        "\n",
        "        print('Iteration ', i+1,' Loss: ',total_loss)\n",
        "        print('Training and Testing Complete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate =  2e-05\n",
            "model intialising...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e53c7aa26aca40e3874c5559abaeb51d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0e8519ae73d4f77b777d9ec9cc8bda1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Set: {'sydneysiege.txt', 'germanwings-crash.txt', 'ottawashooting.txt'}\n",
            "2079\n",
            "size of training data 2327\n",
            "training started....\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.8983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.7480, device='cuda:0')\n",
            "verification training accuracy:  0.664804469273743\n",
            "verification validation accuracy:  0.7786561264822134\n",
            "verification training confusion matrix:  [[912 303]\n",
            " [477 635]]\n",
            "Content classification training accuracy:  0.5384615384615384\n",
            "Content classification validation accuracy:  0.6442687747035574\n",
            "Content classification training confusion matrix:  [[181  89  86  23]\n",
            " [ 96 212 156  33]\n",
            " [ 61 116 220  67]\n",
            " [ 26  72 249 640]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.7071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6594, device='cuda:0')\n",
            "verification training accuracy:  0.7228190803609797\n",
            "verification validation accuracy:  0.7786561264822134\n",
            "verification training confusion matrix:  [[952 263]\n",
            " [382 730]]\n",
            "Content classification training accuracy:  0.7013321873657069\n",
            "Content classification validation accuracy:  0.7154150197628458\n",
            "Content classification training confusion matrix:  [[287  53  26  13]\n",
            " [ 59 309 110  19]\n",
            " [ 52 108 252  52]\n",
            " [ 24  63 116 784]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.6239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6444, device='cuda:0')\n",
            "verification training accuracy:  0.7473141383755909\n",
            "verification validation accuracy:  0.7786561264822134\n",
            "verification training confusion matrix:  [[941 274]\n",
            " [314 798]]\n",
            "Content classification training accuracy:  0.7516115169746455\n",
            "Content classification validation accuracy:  0.7154150197628458\n",
            "Content classification training confusion matrix:  [[305  39  29   6]\n",
            " [ 46 341  94  16]\n",
            " [ 39  81 295  49]\n",
            " [ 22  53 104 808]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6493, device='cuda:0')\n",
            "verification training accuracy:  0.7516115169746455\n",
            "verification validation accuracy:  0.7944664031620553\n",
            "verification training confusion matrix:  [[964 251]\n",
            " [327 785]]\n",
            "Content classification training accuracy:  0.7808336914482166\n",
            "Content classification validation accuracy:  0.7430830039525692\n",
            "Content classification training confusion matrix:  [[319  31  22   7]\n",
            " [ 38 371  71  17]\n",
            " [ 48  74 302  40]\n",
            " [ 23  52  87 825]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6392, device='cuda:0')\n",
            "verification training accuracy:  0.7533304684142673\n",
            "verification validation accuracy:  0.7865612648221344\n",
            "verification training confusion matrix:  [[944 271]\n",
            " [303 809]]\n",
            "Content classification training accuracy:  0.7945853029651913\n",
            "Content classification validation accuracy:  0.7430830039525692\n",
            "Content classification training confusion matrix:  [[319  29  25   6]\n",
            " [ 31 391  61  14]\n",
            " [ 39  61 317  47]\n",
            " [ 20  46  99 822]]\n",
            "Validation accuracy of the model is  0.7687747035573123\n",
            "Now Testing: charliehebdo.txt\n",
            "=================Testing===================\n",
            "charliehebdo.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.94224   0.80506   0.86826      1621\n",
            "         1.0    0.54467   0.82533   0.65625       458\n",
            "\n",
            "    accuracy                        0.80952      2079\n",
            "   macro avg    0.74345   0.81519   0.76226      2079\n",
            "weighted avg    0.85465   0.80952   0.82156      2079\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.64671   0.51675   0.57447       209\n",
            "           1    0.67787   0.64706   0.66211       374\n",
            "           2    0.24691   0.69767   0.36474       172\n",
            "           3    0.95978   0.77492   0.85750      1324\n",
            "\n",
            "    accuracy                        0.71958      2079\n",
            "   macro avg    0.63282   0.65910   0.61470      2079\n",
            "weighted avg    0.81861   0.71958   0.75313      2079\n",
            "\n",
            "Validation accuracy of the model is  0.7648221343873518\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6650, device='cuda:0')\n",
            "verification training accuracy:  0.7675118177911474\n",
            "verification validation accuracy:  0.8023715415019763\n",
            "verification training confusion matrix:  [[970 245]\n",
            " [296 816]]\n",
            "Content classification training accuracy:  0.7885689729265148\n",
            "Content classification validation accuracy:  0.7470355731225297\n",
            "Content classification training confusion matrix:  [[321  26  26   6]\n",
            " [ 32 371  77  17]\n",
            " [ 42  70 311  41]\n",
            " [ 23  57  75 832]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6358, device='cuda:0')\n",
            "verification training accuracy:  0.7700902449505801\n",
            "verification validation accuracy:  0.7984189723320159\n",
            "verification training confusion matrix:  [[983 232]\n",
            " [303 809]]\n",
            "Content classification training accuracy:  0.8001718951439621\n",
            "Content classification validation accuracy:  0.7470355731225297\n",
            "Content classification training confusion matrix:  [[316  32  23   8]\n",
            " [ 37 384  60  16]\n",
            " [ 37  61 323  43]\n",
            " [ 27  39  82 839]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.5559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6399, device='cuda:0')\n",
            "verification training accuracy:  0.7696605070906747\n",
            "verification validation accuracy:  0.7944664031620553\n",
            "verification training confusion matrix:  [[991 224]\n",
            " [312 800]]\n",
            "Content classification training accuracy:  0.7907176622260421\n",
            "Content classification validation accuracy:  0.7509881422924901\n",
            "Content classification training confusion matrix:  [[316  36  22   5]\n",
            " [ 44 383  57  13]\n",
            " [ 46  66 307  45]\n",
            " [ 20  60  73 834]]\n",
            "validation started.. 253\n",
            "Epoch     9: reducing learning rate of group 0 to 2.0000e-06.\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.5573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6563, device='cuda:0')\n",
            "verification training accuracy:  0.7606360120326601\n",
            "verification validation accuracy:  0.7905138339920948\n",
            "verification training confusion matrix:  [[967 248]\n",
            " [309 803]]\n",
            "Content classification training accuracy:  0.8010313708637731\n",
            "Content classification validation accuracy:  0.7193675889328063\n",
            "Content classification training confusion matrix:  [[309  36  21  13]\n",
            " [ 42 391  47  17]\n",
            " [ 44  50 323  47]\n",
            " [ 24  42  80 841]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.4918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6266, device='cuda:0')\n",
            "verification training accuracy:  0.7898581865062312\n",
            "verification validation accuracy:  0.7984189723320159\n",
            "verification training confusion matrix:  [[1037  178]\n",
            " [ 311  801]]\n",
            "Content classification training accuracy:  0.842286205414697\n",
            "Content classification validation accuracy:  0.7628458498023716\n",
            "Content classification training confusion matrix:  [[343  24  10   2]\n",
            " [ 31 406  50  10]\n",
            " [ 40  36 352  36]\n",
            " [ 26  43  59 859]]\n",
            "Validation accuracy of the model is  0.7806324110671937\n",
            "Now Testing: charliehebdo.txt\n",
            "=================Testing===================\n",
            "charliehebdo.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.94180   0.81863   0.87591      1621\n",
            "         1.0    0.56119   0.82096   0.66667       458\n",
            "\n",
            "    accuracy                        0.81914      2079\n",
            "   macro avg    0.75150   0.81980   0.77129      2079\n",
            "weighted avg    0.85796   0.81914   0.82981      2079\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.49827   0.68900   0.57831       209\n",
            "           1    0.63091   0.53476   0.57887       374\n",
            "           2    0.37917   0.52907   0.44175       172\n",
            "           3    0.94242   0.87764   0.90888      1324\n",
            "\n",
            "    accuracy                        0.76816      2079\n",
            "   macro avg    0.61269   0.65762   0.62695      2079\n",
            "weighted avg    0.79513   0.76816   0.77763      2079\n",
            "\n",
            "Validation accuracy of the model is  0.7806324110671937\n",
            "Iteration  10  Loss:  tensor(71.8056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'sydneysiege.txt', 'germanwings-crash.txt', 'charliehebdo.txt'}\n",
            "890\n",
            "size of training data 3397\n",
            "training started....\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.8693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6710, device='cuda:0')\n",
            "verification training accuracy:  0.6935531351192229\n",
            "verification validation accuracy:  0.7580645161290323\n",
            "verification training confusion matrix:  [[2219   78]\n",
            " [ 963  137]]\n",
            "Content classification training accuracy:  0.592581689726229\n",
            "Content classification validation accuracy:  0.7419354838709677\n",
            "Content classification training confusion matrix:  [[ 163  157   54   23]\n",
            " [ 112  390  139   29]\n",
            " [  54  159  170   78]\n",
            " [  24  407  148 1290]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.6420, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6269, device='cuda:0')\n",
            "verification training accuracy:  0.7665587282896673\n",
            "verification validation accuracy:  0.7688172043010753\n",
            "verification training confusion matrix:  [[2093  204]\n",
            " [ 589  511]]\n",
            "Content classification training accuracy:  0.7700912569914631\n",
            "Content classification validation accuracy:  0.793010752688172\n",
            "Content classification training confusion matrix:  [[ 292   62   28   15]\n",
            " [  89  462  102   17]\n",
            " [  43   72  278   68]\n",
            " [  22   89  174 1584]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.5970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6932, device='cuda:0')\n",
            "verification training accuracy:  0.7889314100677068\n",
            "verification validation accuracy:  0.760752688172043\n",
            "verification training confusion matrix:  [[2052  245]\n",
            " [ 472  628]]\n",
            "Content classification training accuracy:  0.7915808065940536\n",
            "Content classification validation accuracy:  0.7849462365591398\n",
            "Content classification training confusion matrix:  [[ 296   49   38   14]\n",
            " [  59  496   99   16]\n",
            " [  37   59  310   55]\n",
            " [  32   75  175 1587]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6165, device='cuda:0')\n",
            "verification training accuracy:  0.7856932587577274\n",
            "verification validation accuracy:  0.7903225806451613\n",
            "verification training confusion matrix:  [[2044  253]\n",
            " [ 475  625]]\n",
            "Content classification training accuracy:  0.8007065057403592\n",
            "Content classification validation accuracy:  0.7661290322580645\n",
            "Content classification training confusion matrix:  [[ 308   51   25   13]\n",
            " [  68  502   86   14]\n",
            " [  32   58  316   55]\n",
            " [  35   71  169 1594]]\n",
            "validation started.. 372\n",
            "Epoch     5: reducing learning rate of group 0 to 2.0000e-06.\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6725, device='cuda:0')\n",
            "verification training accuracy:  0.7895201648513395\n",
            "verification validation accuracy:  0.760752688172043\n",
            "verification training confusion matrix:  [[2021  276]\n",
            " [ 439  661]]\n",
            "Content classification training accuracy:  0.7962908448631145\n",
            "Content classification validation accuracy:  0.7903225806451613\n",
            "Content classification training confusion matrix:  [[ 317   41   28   11]\n",
            " [  64  505   79   22]\n",
            " [  47   58  301   55]\n",
            " [  37   70  180 1582]]\n",
            "Validation accuracy of the model is  0.7809139784946236\n",
            "Now Testing: ottawashooting.txt\n",
            "=================Testing===================\n",
            "ottawashooting.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.62189   0.89286   0.73314       420\n",
            "         1.0    0.84321   0.51489   0.63937       470\n",
            "\n",
            "    accuracy                        0.69326       890\n",
            "   macro avg    0.73255   0.70388   0.68625       890\n",
            "weighted avg    0.73876   0.69326   0.68362       890\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.74843   0.62963   0.68391       189\n",
            "           1    0.50000   0.65385   0.56667       182\n",
            "           2    0.56024   0.53143   0.54545       175\n",
            "           3    0.86239   0.81977   0.84054       344\n",
            "\n",
            "    accuracy                        0.68876       890\n",
            "   macro avg    0.66776   0.65867   0.65914       890\n",
            "weighted avg    0.70467   0.68876   0.69325       890\n",
            "\n",
            "Validation accuracy of the model is  0.7755376344086021\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6029, device='cuda:0')\n",
            "verification training accuracy:  0.8015896379158081\n",
            "verification validation accuracy:  0.7849462365591398\n",
            "verification training confusion matrix:  [[2061  236]\n",
            " [ 438  662]]\n",
            "Content classification training accuracy:  0.8398586988519282\n",
            "Content classification validation accuracy:  0.7661290322580645\n",
            "Content classification training confusion matrix:  [[ 316   42   26   13]\n",
            " [  50  549   59   12]\n",
            " [  28   50  337   46]\n",
            " [  30   57  131 1651]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6161, device='cuda:0')\n",
            "verification training accuracy:  0.8098322048866647\n",
            "verification validation accuracy:  0.7849462365591398\n",
            "verification training confusion matrix:  [[2064  233]\n",
            " [ 413  687]]\n",
            "Content classification training accuracy:  0.8401530762437445\n",
            "Content classification validation accuracy:  0.7849462365591398\n",
            "Content classification training confusion matrix:  [[ 331   35   21   10]\n",
            " [  55  534   68   13]\n",
            " [  32   40  341   48]\n",
            " [  33   53  135 1648]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.4964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6010, device='cuda:0')\n",
            "verification training accuracy:  0.8130703561966441\n",
            "verification validation accuracy:  0.782258064516129\n",
            "verification training confusion matrix:  [[2064  233]\n",
            " [ 402  698]]\n",
            "Content classification training accuracy:  0.8513394171327642\n",
            "Content classification validation accuracy:  0.793010752688172\n",
            "Content classification training confusion matrix:  [[ 330   34   23   10]\n",
            " [  47  552   60   11]\n",
            " [  34   32  350   45]\n",
            " [  22   64  123 1660]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.4981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6135, device='cuda:0')\n",
            "verification training accuracy:  0.8118928466293789\n",
            "verification validation accuracy:  0.7849462365591398\n",
            "verification training confusion matrix:  [[2047  250]\n",
            " [ 389  711]]\n",
            "Content classification training accuracy:  0.8481012658227848\n",
            "Content classification validation accuracy:  0.7768817204301075\n",
            "Content classification training confusion matrix:  [[ 334   33   20   10]\n",
            " [  51  556   52   11]\n",
            " [  29   42  340   50]\n",
            " [  31   61  126 1651]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.4935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6175, device='cuda:0')\n",
            "verification training accuracy:  0.810126582278481\n",
            "verification validation accuracy:  0.7849462365591398\n",
            "verification training confusion matrix:  [[2063  234]\n",
            " [ 411  689]]\n",
            "Content classification training accuracy:  0.8478068884309685\n",
            "Content classification validation accuracy:  0.793010752688172\n",
            "Content classification training confusion matrix:  [[ 334   32   21   10]\n",
            " [  49  547   60   14]\n",
            " [  35   36  353   37]\n",
            " [  25   52  146 1646]]\n",
            "Validation accuracy of the model is  0.7889784946236559\n",
            "Now Testing: ottawashooting.txt\n",
            "=================Testing===================\n",
            "ottawashooting.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.65741   0.84524   0.73958       420\n",
            "         1.0    0.81429   0.60638   0.69512       470\n",
            "\n",
            "    accuracy                        0.71910       890\n",
            "   macro avg    0.73585   0.72581   0.71735       890\n",
            "weighted avg    0.74025   0.71910   0.71610       890\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.71348   0.67196   0.69210       189\n",
            "           1    0.52381   0.60440   0.56122       182\n",
            "           2    0.52657   0.62286   0.57068       175\n",
            "           3    0.90508   0.77616   0.83568       344\n",
            "\n",
            "    accuracy                        0.68876       890\n",
            "   macro avg    0.66724   0.66884   0.66492       890\n",
            "weighted avg    0.71200   0.68876   0.69696       890\n",
            "\n",
            "Validation accuracy of the model is  0.7889784946236559\n",
            "Iteration  10  Loss:  tensor(105.1114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'charliehebdo.txt', 'sydneysiege.txt', 'ottawashooting.txt'}\n",
            "469\n",
            "size of training data 3776\n",
            "training started....\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.8097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6823, device='cuda:0')\n",
            "verification training accuracy:  0.7084216101694916\n",
            "verification validation accuracy:  0.7367149758454107\n",
            "verification training confusion matrix:  [[2367  108]\n",
            " [ 993  308]]\n",
            "Content classification training accuracy:  0.6644597457627118\n",
            "Content classification validation accuracy:  0.7657004830917874\n",
            "Content classification training confusion matrix:  [[ 351   76   29   42]\n",
            " [ 199  356  107   48]\n",
            " [  99   92  239   87]\n",
            " [ 155   95  238 1563]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.6249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6053, device='cuda:0')\n",
            "verification training accuracy:  0.7714512711864406\n",
            "verification validation accuracy:  0.7898550724637681\n",
            "verification training confusion matrix:  [[2242  233]\n",
            " [ 630  671]]\n",
            "Content classification training accuracy:  0.7714512711864406\n",
            "Content classification validation accuracy:  0.7898550724637681\n",
            "Content classification training confusion matrix:  [[ 377   64   33   24]\n",
            " [  87  487  119   17]\n",
            " [  44   74  336   63]\n",
            " [  49   83  206 1713]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.5788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.7135, device='cuda:0')\n",
            "verification training accuracy:  0.7923728813559322\n",
            "verification validation accuracy:  0.7681159420289855\n",
            "verification training confusion matrix:  [[2198  277]\n",
            " [ 507  794]]\n",
            "Content classification training accuracy:  0.7929025423728814\n",
            "Content classification validation accuracy:  0.7439613526570048\n",
            "Content classification training confusion matrix:  [[ 397   56   29   16]\n",
            " [  79  503  108   20]\n",
            " [  49   66  339   63]\n",
            " [  45   77  174 1755]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6002, device='cuda:0')\n",
            "verification training accuracy:  0.7884004237288136\n",
            "verification validation accuracy:  0.7995169082125604\n",
            "verification training confusion matrix:  [[2194  281]\n",
            " [ 518  783]]\n",
            "Content classification training accuracy:  0.7897245762711864\n",
            "Content classification validation accuracy:  0.7439613526570048\n",
            "Content classification training confusion matrix:  [[ 392   57   33   16]\n",
            " [  83  506  101   20]\n",
            " [  36   68  350   63]\n",
            " [  49   69  199 1734]]\n",
            "validation started.. 414\n",
            "Epoch     5: reducing learning rate of group 0 to 2.0000e-06.\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6479, device='cuda:0')\n",
            "verification training accuracy:  0.7971398305084746\n",
            "verification validation accuracy:  0.7777777777777778\n",
            "verification training confusion matrix:  [[2182  293]\n",
            " [ 473  828]]\n",
            "Content classification training accuracy:  0.7979343220338984\n",
            "Content classification validation accuracy:  0.7101449275362319\n",
            "Content classification training confusion matrix:  [[ 392   56   33   17]\n",
            " [  87  517   88   18]\n",
            " [  34   69  355   59]\n",
            " [  52   65  185 1749]]\n",
            "Validation accuracy of the model is  0.7898550724637681\n",
            "Now Testing: germanwings-crash.txt\n",
            "=================Testing===================\n",
            "germanwings-crash.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.67284   0.47186   0.55471       231\n",
            "         1.0    0.60261   0.77731   0.67890       238\n",
            "\n",
            "    accuracy                        0.62687       469\n",
            "   macro avg    0.63772   0.62459   0.61680       469\n",
            "weighted avg    0.63720   0.62687   0.61773       469\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.40123   0.84416   0.54393        77\n",
            "           1    0.62500   0.14599   0.23669       137\n",
            "           2    0.36598   0.62832   0.46254       113\n",
            "           3    0.90123   0.51408   0.65471       142\n",
            "\n",
            "    accuracy                        0.48827       469\n",
            "   macro avg    0.57336   0.53314   0.47447       469\n",
            "weighted avg    0.60949   0.48827   0.46811       469\n",
            "\n",
            "Validation accuracy of the model is  0.7439613526570048\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6092, device='cuda:0')\n",
            "verification training accuracy:  0.807468220338983\n",
            "verification validation accuracy:  0.8115942028985508\n",
            "verification training confusion matrix:  [[2220  255]\n",
            " [ 472  829]]\n",
            "Content classification training accuracy:  0.8347457627118644\n",
            "Content classification validation accuracy:  0.7801932367149759\n",
            "Content classification training confusion matrix:  [[ 416   48   25    9]\n",
            " [  55  555   92    8]\n",
            " [  36   45  392   44]\n",
            " [  55   54  153 1789]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.4934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6115, device='cuda:0')\n",
            "verification training accuracy:  0.8183262711864406\n",
            "verification validation accuracy:  0.7801932367149759\n",
            "verification training confusion matrix:  [[2218  257]\n",
            " [ 429  872]]\n",
            "Content classification training accuracy:  0.841896186440678\n",
            "Content classification validation accuracy:  0.7898550724637681\n",
            "Content classification training confusion matrix:  [[ 431   42   17    8]\n",
            " [  53  565   79   13]\n",
            " [  32   53  379   53]\n",
            " [  52   53  142 1804]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.4903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5887, device='cuda:0')\n",
            "verification training accuracy:  0.8209745762711864\n",
            "verification validation accuracy:  0.7995169082125604\n",
            "verification training confusion matrix:  [[2217  258]\n",
            " [ 418  883]]\n",
            "Content classification training accuracy:  0.84375\n",
            "Content classification validation accuracy:  0.7705314009661836\n",
            "Content classification training confusion matrix:  [[ 420   44   19   15]\n",
            " [  55  561   84   10]\n",
            " [  35   47  389   46]\n",
            " [  50   48  137 1816]]\n",
            "validation started.. 414\n",
            "Epoch     9: reducing learning rate of group 0 to 2.0000e-07.\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.4843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5984, device='cuda:0')\n",
            "verification training accuracy:  0.817531779661017\n",
            "verification validation accuracy:  0.7995169082125604\n",
            "verification training confusion matrix:  [[2214  261]\n",
            " [ 428  873]]\n",
            "Content classification training accuracy:  0.8429555084745762\n",
            "Content classification validation accuracy:  0.7874396135265701\n",
            "Content classification training confusion matrix:  [[ 428   40   16   14]\n",
            " [  59  559   81   11]\n",
            " [  30   56  388   43]\n",
            " [  48   57  138 1808]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.4777, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6022, device='cuda:0')\n",
            "verification training accuracy:  0.8233580508474576\n",
            "verification validation accuracy:  0.7971014492753623\n",
            "verification training confusion matrix:  [[2225  250]\n",
            " [ 417  884]]\n",
            "Content classification training accuracy:  0.8551377118644068\n",
            "Content classification validation accuracy:  0.7898550724637681\n",
            "Content classification training confusion matrix:  [[ 431   39   15   13]\n",
            " [  52  569   79   10]\n",
            " [  28   45  401   43]\n",
            " [  45   45  133 1828]]\n",
            "Validation accuracy of the model is  0.7958937198067633\n",
            "Now Testing: germanwings-crash.txt\n",
            "=================Testing===================\n",
            "germanwings-crash.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.70229   0.39827   0.50829       231\n",
            "         1.0    0.58876   0.83613   0.69097       238\n",
            "\n",
            "    accuracy                        0.62047       469\n",
            "   macro avg    0.64552   0.61720   0.59963       469\n",
            "weighted avg    0.64468   0.62047   0.60099       469\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.30172   0.90909   0.45307        77\n",
            "           1    0.57971   0.29197   0.38835       137\n",
            "           2    0.46067   0.36283   0.40594       113\n",
            "           3    0.91139   0.50704   0.65158       142\n",
            "\n",
            "    accuracy                        0.47548       469\n",
            "   macro avg    0.56338   0.51773   0.47474       469\n",
            "weighted avg    0.60581   0.47548   0.48291       469\n",
            "\n",
            "Validation accuracy of the model is  0.7934782608695652\n",
            "Iteration  10  Loss:  tensor(112.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'charliehebdo.txt', 'germanwings-crash.txt', 'ottawashooting.txt'}\n",
            "1221\n",
            "size of training data 3100\n",
            "training started....\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.8715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6638, device='cuda:0')\n",
            "verification training accuracy:  0.6935483870967742\n",
            "verification validation accuracy:  0.7514792899408284\n",
            "verification training confusion matrix:  [[1962   91]\n",
            " [ 859  188]]\n",
            "Content classification training accuracy:  0.5948387096774194\n",
            "Content classification validation accuracy:  0.7544378698224852\n",
            "Content classification training confusion matrix:  [[ 303   83   24   20]\n",
            " [ 185  309   97   34]\n",
            " [  93  125  125   72]\n",
            " [ 221  182  120 1107]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.6374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6153, device='cuda:0')\n",
            "verification training accuracy:  0.7816129032258065\n",
            "verification validation accuracy:  0.757396449704142\n",
            "verification training confusion matrix:  [[1882  171]\n",
            " [ 506  541]]\n",
            "Content classification training accuracy:  0.7577419354838709\n",
            "Content classification validation accuracy:  0.7928994082840237\n",
            "Content classification training confusion matrix:  [[ 331   53   30   16]\n",
            " [  80  415  117   13]\n",
            " [  33   80  239   63]\n",
            " [  47   61  158 1364]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.5767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5683, device='cuda:0')\n",
            "verification training accuracy:  0.7925806451612903\n",
            "verification validation accuracy:  0.8284023668639053\n",
            "verification training confusion matrix:  [[1871  182]\n",
            " [ 461  586]]\n",
            "Content classification training accuracy:  0.787741935483871\n",
            "Content classification validation accuracy:  0.7603550295857988\n",
            "Content classification training confusion matrix:  [[ 341   40   33   16]\n",
            " [  56  447  110   12]\n",
            " [  24   65  274   52]\n",
            " [  41   70  139 1380]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6013, device='cuda:0')\n",
            "verification training accuracy:  0.8125806451612904\n",
            "verification validation accuracy:  0.8047337278106509\n",
            "verification training confusion matrix:  [[1851  202]\n",
            " [ 379  668]]\n",
            "Content classification training accuracy:  0.7941935483870968\n",
            "Content classification validation accuracy:  0.7781065088757396\n",
            "Content classification training confusion matrix:  [[ 340   51   24   15]\n",
            " [  66  467   85    7]\n",
            " [  32   61  268   54]\n",
            " [  47   61  135 1387]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5879, device='cuda:0')\n",
            "verification training accuracy:  0.8183870967741935\n",
            "verification validation accuracy:  0.8284023668639053\n",
            "verification training confusion matrix:  [[1871  182]\n",
            " [ 381  666]]\n",
            "Content classification training accuracy:  0.8006451612903226\n",
            "Content classification validation accuracy:  0.7337278106508875\n",
            "Content classification training confusion matrix:  [[ 349   40   23   18]\n",
            " [  50  470   94   11]\n",
            " [  25   59  281   50]\n",
            " [  47   49  152 1382]]\n",
            "Validation accuracy of the model is  0.7943786982248521\n",
            "Now Testing: sydneysiege.txt\n",
            "=================Testing===================\n",
            "sydneysiege.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.75607   0.84692   0.79892       699\n",
            "         1.0    0.75571   0.63410   0.68958       522\n",
            "\n",
            "    accuracy                        0.75594      1221\n",
            "   macro avg    0.75589   0.74051   0.74425      1221\n",
            "weighted avg    0.75591   0.75594   0.75218      1221\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.52381   0.35948   0.42636       153\n",
            "           1    0.43562   0.68534   0.53266       232\n",
            "           2    0.45415   0.46018   0.45714       226\n",
            "           3    0.92720   0.79344   0.85512       610\n",
            "\n",
            "    accuracy                        0.65684      1221\n",
            "   macro avg    0.58519   0.57461   0.56782      1221\n",
            "weighted avg    0.69569   0.65684   0.66646      1221\n",
            "\n",
            "Validation accuracy of the model is  0.7810650887573964\n",
            "validation started.. 338\n",
            "Epoch     6: reducing learning rate of group 0 to 2.0000e-06.\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6973, device='cuda:0')\n",
            "verification training accuracy:  0.8148387096774193\n",
            "verification validation accuracy:  0.7869822485207101\n",
            "verification training confusion matrix:  [[1840  213]\n",
            " [ 361  686]]\n",
            "Content classification training accuracy:  0.8025806451612904\n",
            "Content classification validation accuracy:  0.7781065088757396\n",
            "Content classification training confusion matrix:  [[ 347   38   24   21]\n",
            " [  53  479   76   17]\n",
            " [  27   61  279   48]\n",
            " [  49   58  140 1383]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.4833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5515, device='cuda:0')\n",
            "verification training accuracy:  0.8351612903225807\n",
            "verification validation accuracy:  0.8106508875739645\n",
            "verification training confusion matrix:  [[1859  194]\n",
            " [ 317  730]]\n",
            "Content classification training accuracy:  0.8403225806451613\n",
            "Content classification validation accuracy:  0.8136094674556213\n",
            "Content classification training confusion matrix:  [[ 352   32   27   19]\n",
            " [  45  517   56    7]\n",
            " [  24   45  302   44]\n",
            " [  36   45  115 1434]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.4673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5494, device='cuda:0')\n",
            "verification training accuracy:  0.8351612903225807\n",
            "verification validation accuracy:  0.8136094674556213\n",
            "verification training confusion matrix:  [[1876  177]\n",
            " [ 334  713]]\n",
            "Content classification training accuracy:  0.8529032258064516\n",
            "Content classification validation accuracy:  0.8017751479289941\n",
            "Content classification training confusion matrix:  [[ 360   39   17   14]\n",
            " [  36  520   63    6]\n",
            " [  22   38  324   31]\n",
            " [  42   46  102 1440]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.4631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5365, device='cuda:0')\n",
            "verification training accuracy:  0.8435483870967742\n",
            "verification validation accuracy:  0.8224852071005917\n",
            "verification training confusion matrix:  [[1878  175]\n",
            " [ 310  737]]\n",
            "Content classification training accuracy:  0.8564516129032258\n",
            "Content classification validation accuracy:  0.7899408284023669\n",
            "Content classification training confusion matrix:  [[ 361   32   24   13]\n",
            " [  35  524   59    7]\n",
            " [  20   37  332   26]\n",
            " [  37   42  113 1438]]\n",
            "validation started.. 338\n",
            "Epoch    10: reducing learning rate of group 0 to 2.0000e-07.\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.4587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5479, device='cuda:0')\n",
            "verification training accuracy:  0.8416129032258064\n",
            "verification validation accuracy:  0.8195266272189349\n",
            "verification training confusion matrix:  [[1877  176]\n",
            " [ 315  732]]\n",
            "Content classification training accuracy:  0.85\n",
            "Content classification validation accuracy:  0.7958579881656804\n",
            "Content classification training confusion matrix:  [[ 362   36   22   10]\n",
            " [  35  520   62    8]\n",
            " [  19   39  327   30]\n",
            " [  37   50  117 1426]]\n",
            "Validation accuracy of the model is  0.8121301775147929\n",
            "Now Testing: sydneysiege.txt\n",
            "=================Testing===================\n",
            "sydneysiege.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.68750   0.91273   0.78427       699\n",
            "         1.0    0.79181   0.44444   0.56933       522\n",
            "\n",
            "    accuracy                        0.71253      1221\n",
            "   macro avg    0.73965   0.67859   0.67680      1221\n",
            "weighted avg    0.73209   0.71253   0.69237      1221\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.68333   0.26797   0.38498       153\n",
            "           1    0.46742   0.71121   0.56410       232\n",
            "           2    0.56650   0.50885   0.53613       226\n",
            "           3    0.88430   0.87705   0.88066       610\n",
            "\n",
            "    accuracy                        0.70106      1221\n",
            "   macro avg    0.65039   0.59127   0.59147      1221\n",
            "weighted avg    0.72108   0.70106   0.69463      1221\n",
            "\n",
            "Validation accuracy of the model is  0.8076923076923077\n",
            "Iteration  10  Loss:  tensor(88.9838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "learning rate =  1e-05\n",
            "model intialising...\n",
            "Training Set: {'sydneysiege.txt', 'germanwings-crash.txt', 'ottawashooting.txt'}\n",
            "2079\n",
            "size of training data 2327\n",
            "training started....\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.9855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.8290, device='cuda:0')\n",
            "verification training accuracy:  0.6312849162011173\n",
            "verification validation accuracy:  0.7193675889328063\n",
            "verification training confusion matrix:  [[984 231]\n",
            " [627 485]]\n",
            "Content classification training accuracy:  0.400945423291792\n",
            "Content classification validation accuracy:  0.6086956521739131\n",
            "Content classification training confusion matrix:  [[296  38  23  22]\n",
            " [345  77  49  26]\n",
            " [234  80  82  68]\n",
            " [342 119  48 478]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.7760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.7056, device='cuda:0')\n",
            "verification training accuracy:  0.7133648474430597\n",
            "verification validation accuracy:  0.7707509881422925\n",
            "verification training confusion matrix:  [[996 219]\n",
            " [448 664]]\n",
            "Content classification training accuracy:  0.6781263429308122\n",
            "Content classification validation accuracy:  0.7114624505928854\n",
            "Content classification training confusion matrix:  [[298  28  39  14]\n",
            " [117 197 152  31]\n",
            " [ 73  47 281  63]\n",
            " [ 35  30 120 802]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.6840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6576, device='cuda:0')\n",
            "verification training accuracy:  0.7331327889987108\n",
            "verification validation accuracy:  0.7905138339920948\n",
            "verification training confusion matrix:  [[978 237]\n",
            " [384 728]]\n",
            "Content classification training accuracy:  0.7185217017619252\n",
            "Content classification validation accuracy:  0.7351778656126482\n",
            "Content classification training confusion matrix:  [[305  38  28   8]\n",
            " [ 58 309 109  21]\n",
            " [ 58  90 256  60]\n",
            " [ 19  59 107 802]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.6363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6360, device='cuda:0')\n",
            "verification training accuracy:  0.7468844005156854\n",
            "verification validation accuracy:  0.8063241106719368\n",
            "verification training confusion matrix:  [[968 247]\n",
            " [342 770]]\n",
            "Content classification training accuracy:  0.7653631284916201\n",
            "Content classification validation accuracy:  0.7075098814229249\n",
            "Content classification training confusion matrix:  [[314  34  24   7]\n",
            " [ 43 353  81  20]\n",
            " [ 48  78 287  51]\n",
            " [ 22  52  86 827]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6301, device='cuda:0')\n",
            "verification training accuracy:  0.7550494198538892\n",
            "verification validation accuracy:  0.7905138339920948\n",
            "verification training confusion matrix:  [[975 240]\n",
            " [330 782]]\n",
            "Content classification training accuracy:  0.7782552642887839\n",
            "Content classification validation accuracy:  0.758893280632411\n",
            "Content classification training confusion matrix:  [[313  38  23   5]\n",
            " [ 39 367  79  12]\n",
            " [ 43  69 306  46]\n",
            " [ 21  50  91 825]]\n",
            "Validation accuracy of the model is  0.7747035573122529\n",
            "Now Testing: charliehebdo.txt\n",
            "=================Testing===================\n",
            "charliehebdo.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.94126   0.81061   0.87106      1621\n",
            "         1.0    0.55051   0.82096   0.65907       458\n",
            "\n",
            "    accuracy                        0.81289      2079\n",
            "   macro avg    0.74589   0.81579   0.76507      2079\n",
            "weighted avg    0.85518   0.81289   0.82436      2079\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.57255   0.69856   0.62931       209\n",
            "           1    0.58432   0.65775   0.61887       374\n",
            "           2    0.35263   0.38953   0.37017       172\n",
            "           3    0.95136   0.87160   0.90974      1324\n",
            "\n",
            "    accuracy                        0.77585      2079\n",
            "   macro avg    0.61522   0.65436   0.63202      2079\n",
            "weighted avg    0.79772   0.77585   0.78458      2079\n",
            "\n",
            "Validation accuracy of the model is  0.7747035573122529\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6239, device='cuda:0')\n",
            "verification training accuracy:  0.7533304684142673\n",
            "verification validation accuracy:  0.7944664031620553\n",
            "verification training confusion matrix:  [[946 269]\n",
            " [305 807]]\n",
            "Content classification training accuracy:  0.8023205844434895\n",
            "Content classification validation accuracy:  0.7312252964426877\n",
            "Content classification training confusion matrix:  [[326  27  20   6]\n",
            " [ 32 379  71  15]\n",
            " [ 44  55 324  41]\n",
            " [ 17  58  74 838]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6282, device='cuda:0')\n",
            "verification training accuracy:  0.7606360120326601\n",
            "verification validation accuracy:  0.8023715415019763\n",
            "verification training confusion matrix:  [[963 252]\n",
            " [305 807]]\n",
            "Content classification training accuracy:  0.8096261280618823\n",
            "Content classification validation accuracy:  0.7430830039525692\n",
            "Content classification training confusion matrix:  [[333  25  17   4]\n",
            " [ 34 389  62  12]\n",
            " [ 43  60 319  42]\n",
            " [ 20  51  73 843]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.5435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6081, device='cuda:0')\n",
            "verification training accuracy:  0.7778255264288784\n",
            "verification validation accuracy:  0.8023715415019763\n",
            "verification training confusion matrix:  [[990 225]\n",
            " [292 820]]\n",
            "Content classification training accuracy:  0.8130640309411259\n",
            "Content classification validation accuracy:  0.7509881422924901\n",
            "Content classification training confusion matrix:  [[336  22  16   5]\n",
            " [ 37 378  60  22]\n",
            " [ 43  54 327  40]\n",
            " [ 15  46  75 851]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.5404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6294, device='cuda:0')\n",
            "verification training accuracy:  0.7713794585302965\n",
            "verification validation accuracy:  0.8023715415019763\n",
            "verification training confusion matrix:  [[991 224]\n",
            " [308 804]]\n",
            "Content classification training accuracy:  0.812204555221315\n",
            "Content classification validation accuracy:  0.7351778656126482\n",
            "Content classification training confusion matrix:  [[320  38  14   7]\n",
            " [ 31 391  60  15]\n",
            " [ 45  54 330  35]\n",
            " [ 22  46  70 849]]\n",
            "validation started.. 253\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.5104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6457, device='cuda:0')\n",
            "verification training accuracy:  0.7804039535883112\n",
            "verification validation accuracy:  0.8063241106719368\n",
            "verification training confusion matrix:  [[997 218]\n",
            " [293 819]]\n",
            "Content classification training accuracy:  0.8336914482165879\n",
            "Content classification validation accuracy:  0.7430830039525692\n",
            "Content classification training confusion matrix:  [[345  19  14   1]\n",
            " [ 27 401  60   9]\n",
            " [ 45  49 331  39]\n",
            " [ 18  41  65 863]]\n",
            "Validation accuracy of the model is  0.7766798418972332\n",
            "Now Testing: charliehebdo.txt\n",
            "=================Testing===================\n",
            "charliehebdo.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.93329   0.82850   0.87778      1621\n",
            "         1.0    0.56563   0.79039   0.65938       458\n",
            "\n",
            "    accuracy                        0.82011      2079\n",
            "   macro avg    0.74946   0.80945   0.76858      2079\n",
            "weighted avg    0.85229   0.82011   0.82967      2079\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.55147   0.71770   0.62370       209\n",
            "           1    0.63128   0.60428   0.61749       374\n",
            "           2    0.36564   0.48256   0.41604       172\n",
            "           3    0.94435   0.87160   0.90652      1324\n",
            "\n",
            "    accuracy                        0.77585      2079\n",
            "   macro avg    0.62319   0.66904   0.64094      2079\n",
            "weighted avg    0.80066   0.77585   0.78551      2079\n",
            "\n",
            "Validation accuracy of the model is  0.7747035573122529\n",
            "Iteration  10  Loss:  tensor(74.5199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'sydneysiege.txt', 'germanwings-crash.txt', 'charliehebdo.txt'}\n",
            "890\n",
            "size of training data 3397\n",
            "training started....\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.9871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.8069, device='cuda:0')\n",
            "verification training accuracy:  0.6782455107447748\n",
            "verification validation accuracy:  0.7311827956989247\n",
            "verification training confusion matrix:  [[2286   11]\n",
            " [1082   18]]\n",
            "Content classification training accuracy:  0.4851339417132764\n",
            "Content classification validation accuracy:  0.706989247311828\n",
            "Content classification training confusion matrix:  [[ 181  110   51   55]\n",
            " [ 243  261   85   81]\n",
            " [ 160   83  133   85]\n",
            " [ 361  306  129 1073]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.7342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6732, device='cuda:0')\n",
            "verification training accuracy:  0.7382984986753017\n",
            "verification validation accuracy:  0.7553763440860215\n",
            "verification training confusion matrix:  [[2158  139]\n",
            " [ 750  350]]\n",
            "Content classification training accuracy:  0.730939063879894\n",
            "Content classification validation accuracy:  0.7473118279569892\n",
            "Content classification training confusion matrix:  [[ 233   94   49   21]\n",
            " [ 119  420  110   21]\n",
            " [  57   76  249   79]\n",
            " [  33   80  175 1581]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.6422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6321, device='cuda:0')\n",
            "verification training accuracy:  0.7659699735060347\n",
            "verification validation accuracy:  0.760752688172043\n",
            "verification training confusion matrix:  [[2059  238]\n",
            " [ 557  543]]\n",
            "Content classification training accuracy:  0.7851045039740948\n",
            "Content classification validation accuracy:  0.7688172043010753\n",
            "Content classification training confusion matrix:  [[ 285   60   37   15]\n",
            " [  79  483   96   12]\n",
            " [  39   78  277   67]\n",
            " [  24   77  146 1622]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6253, device='cuda:0')\n",
            "verification training accuracy:  0.7774506917868708\n",
            "verification validation accuracy:  0.7553763440860215\n",
            "verification training confusion matrix:  [[2067  230]\n",
            " [ 526  574]]\n",
            "Content classification training accuracy:  0.8039446570503386\n",
            "Content classification validation accuracy:  0.8010752688172043\n",
            "Content classification training confusion matrix:  [[ 302   50   33   12]\n",
            " [  75  487   93   15]\n",
            " [  46   50  306   59]\n",
            " [  26   71  136 1636]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6234, device='cuda:0')\n",
            "verification training accuracy:  0.7806888430968502\n",
            "verification validation accuracy:  0.7634408602150538\n",
            "verification training confusion matrix:  [[2065  232]\n",
            " [ 513  587]]\n",
            "Content classification training accuracy:  0.8065940535766853\n",
            "Content classification validation accuracy:  0.7983870967741935\n",
            "Content classification training confusion matrix:  [[ 309   42   31   15]\n",
            " [  68  508   76   18]\n",
            " [  38   68  302   53]\n",
            " [  27   72  149 1621]]\n",
            "Validation accuracy of the model is  0.7809139784946236\n",
            "Now Testing: ottawashooting.txt\n",
            "=================Testing===================\n",
            "ottawashooting.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.60592   0.92619   0.73258       420\n",
            "         1.0    0.87500   0.46170   0.60446       470\n",
            "\n",
            "    accuracy                        0.68090       890\n",
            "   macro avg    0.74046   0.69395   0.66852       890\n",
            "weighted avg    0.74802   0.68090   0.66492       890\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.70000   0.66667   0.68293       189\n",
            "           1    0.57862   0.50549   0.53959       182\n",
            "           2    0.54315   0.61143   0.57527       175\n",
            "           3    0.82203   0.84593   0.83381       344\n",
            "\n",
            "    accuracy                        0.69213       890\n",
            "   macro avg    0.66095   0.65738   0.65790       890\n",
            "weighted avg    0.69150   0.69213   0.69077       890\n",
            "\n",
            "Validation accuracy of the model is  0.7809139784946236\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6148, device='cuda:0')\n",
            "verification training accuracy:  0.7927583161613189\n",
            "verification validation accuracy:  0.7634408602150538\n",
            "verification training confusion matrix:  [[2050  247]\n",
            " [ 457  643]]\n",
            "Content classification training accuracy:  0.8083603179275831\n",
            "Content classification validation accuracy:  0.7903225806451613\n",
            "Content classification training confusion matrix:  [[ 310   43   33   11]\n",
            " [  70  501   85   14]\n",
            " [  41   55  317   48]\n",
            " [  27   72  152 1618]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5905, device='cuda:0')\n",
            "verification training accuracy:  0.7930526935531351\n",
            "verification validation accuracy:  0.7849462365591398\n",
            "verification training confusion matrix:  [[2036  261]\n",
            " [ 442  658]]\n",
            "Content classification training accuracy:  0.8166028848984398\n",
            "Content classification validation accuracy:  0.771505376344086\n",
            "Content classification training confusion matrix:  [[ 312   38   33   14]\n",
            " [  53  515   87   15]\n",
            " [  40   41  326   54]\n",
            " [  29   60  159 1621]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.5357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6131, device='cuda:0')\n",
            "verification training accuracy:  0.7962908448631145\n",
            "verification validation accuracy:  0.782258064516129\n",
            "verification training confusion matrix:  [[2031  266]\n",
            " [ 426  674]]\n",
            "Content classification training accuracy:  0.8266117162201942\n",
            "Content classification validation accuracy:  0.8145161290322581\n",
            "Content classification training confusion matrix:  [[ 316   39   29   13]\n",
            " [  53  534   69   14]\n",
            " [  38   44  329   50]\n",
            " [  32   71  137 1629]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.5402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6392, device='cuda:0')\n",
            "verification training accuracy:  0.797762731822196\n",
            "verification validation accuracy:  0.782258064516129\n",
            "verification training confusion matrix:  [[2049  248]\n",
            " [ 439  661]]\n",
            "Content classification training accuracy:  0.826317338828378\n",
            "Content classification validation accuracy:  0.7661290322580645\n",
            "Content classification training confusion matrix:  [[ 319   43   22   13]\n",
            " [  57  532   65   16]\n",
            " [  38   43  323   57]\n",
            " [  34   63  139 1633]]\n",
            "validation started.. 372\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.5420, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6473, device='cuda:0')\n",
            "verification training accuracy:  0.8036502796585222\n",
            "verification validation accuracy:  0.7983870967741935\n",
            "verification training confusion matrix:  [[2057  240]\n",
            " [ 427  673]]\n",
            "Content classification training accuracy:  0.822490432734766\n",
            "Content classification validation accuracy:  0.717741935483871\n",
            "Content classification training confusion matrix:  [[ 317   41   26   13]\n",
            " [  71  510   74   15]\n",
            " [  44   41  323   53]\n",
            " [  24   73  128 1644]]\n",
            "Validation accuracy of the model is  0.7983870967741935\n",
            "Now Testing: ottawashooting.txt\n",
            "=================Testing===================\n",
            "ottawashooting.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.63176   0.89048   0.73913       420\n",
            "         1.0    0.84564   0.53617   0.65625       470\n",
            "\n",
            "    accuracy                        0.70337       890\n",
            "   macro avg    0.73870   0.71332   0.69769       890\n",
            "weighted avg    0.74471   0.70337   0.69536       890\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.77273   0.62963   0.69388       189\n",
            "           1    0.54505   0.66484   0.59901       182\n",
            "           2    0.56354   0.58286   0.57303       175\n",
            "           3    0.85285   0.82558   0.83900       344\n",
            "\n",
            "    accuracy                        0.70337       890\n",
            "   macro avg    0.68354   0.67573   0.67623       890\n",
            "weighted avg    0.71600   0.70337   0.70681       890\n",
            "\n",
            "Validation accuracy of the model is  0.7580645161290323\n",
            "Iteration  10  Loss:  tensor(115.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'charliehebdo.txt', 'sydneysiege.txt', 'ottawashooting.txt'}\n",
            "469\n",
            "size of training data 3776\n",
            "training started....\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.9473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.7594, device='cuda:0')\n",
            "verification training accuracy:  0.6273834745762712\n",
            "verification validation accuracy:  0.7198067632850241\n",
            "verification training confusion matrix:  [[2093  382]\n",
            " [1025  276]]\n",
            "Content classification training accuracy:  0.4856991525423729\n",
            "Content classification validation accuracy:  0.7125603864734299\n",
            "Content classification training confusion matrix:  [[ 294  130   42   32]\n",
            " [ 349  210  123   28]\n",
            " [ 186   92  172   67]\n",
            " [ 634   94  165 1158]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.6984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6556, device='cuda:0')\n",
            "verification training accuracy:  0.7494703389830508\n",
            "verification validation accuracy:  0.7608695652173914\n",
            "verification training confusion matrix:  [[2306  169]\n",
            " [ 777  524]]\n",
            "Content classification training accuracy:  0.743114406779661\n",
            "Content classification validation accuracy:  0.7391304347826086\n",
            "Content classification training confusion matrix:  [[ 347   81   46   24]\n",
            " [ 125  428  135   22]\n",
            " [  35   87  308   87]\n",
            " [  44   80  204 1723]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.6195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6346, device='cuda:0')\n",
            "verification training accuracy:  0.770656779661017\n",
            "verification validation accuracy:  0.7608695652173914\n",
            "verification training confusion matrix:  [[2205  270]\n",
            " [ 596  705]]\n",
            "Content classification training accuracy:  0.7857521186440678\n",
            "Content classification validation accuracy:  0.782608695652174\n",
            "Content classification training confusion matrix:  [[ 384   61   34   19]\n",
            " [  91  482  119   18]\n",
            " [  37   68  346   66]\n",
            " [  45   72  179 1755]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6321, device='cuda:0')\n",
            "verification training accuracy:  0.7772775423728814\n",
            "verification validation accuracy:  0.7874396135265701\n",
            "verification training confusion matrix:  [[2219  256]\n",
            " [ 585  716]]\n",
            "Content classification training accuracy:  0.7984639830508474\n",
            "Content classification validation accuracy:  0.7681159420289855\n",
            "Content classification training confusion matrix:  [[ 387   63   37   11]\n",
            " [  83  495  113   19]\n",
            " [  34   56  358   69]\n",
            " [  40   71  165 1775]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6681, device='cuda:0')\n",
            "verification training accuracy:  0.788135593220339\n",
            "verification validation accuracy:  0.7729468599033816\n",
            "verification training confusion matrix:  [[2197  278]\n",
            " [ 522  779]]\n",
            "Content classification training accuracy:  0.8040254237288136\n",
            "Content classification validation accuracy:  0.7657004830917874\n",
            "Content classification training confusion matrix:  [[ 397   60   29   12]\n",
            " [  74  499  121   16]\n",
            " [  35   60  361   61]\n",
            " [  43   71  158 1779]]\n",
            "Validation accuracy of the model is  0.7777777777777778\n",
            "Now Testing: germanwings-crash.txt\n",
            "=================Testing===================\n",
            "germanwings-crash.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.69343   0.41126   0.51630       231\n",
            "         1.0    0.59036   0.82353   0.68772       238\n",
            "\n",
            "    accuracy                        0.62047       469\n",
            "   macro avg    0.64190   0.61739   0.60201       469\n",
            "weighted avg    0.64113   0.62047   0.60329       469\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.32692   0.88312   0.47719        77\n",
            "           1    0.51163   0.16058   0.24444       137\n",
            "           2    0.38571   0.47788   0.42688       113\n",
            "           3    0.92308   0.50704   0.65455       142\n",
            "\n",
            "    accuracy                        0.46055       469\n",
            "   macro avg    0.53684   0.50715   0.45077       469\n",
            "weighted avg    0.57554   0.46055   0.45078       469\n",
            "\n",
            "Validation accuracy of the model is  0.7693236714975845\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6104, device='cuda:0')\n",
            "verification training accuracy:  0.7913135593220338\n",
            "verification validation accuracy:  0.7801932367149759\n",
            "verification training confusion matrix:  [[2193  282]\n",
            " [ 506  795]]\n",
            "Content classification training accuracy:  0.8077330508474576\n",
            "Content classification validation accuracy:  0.7681159420289855\n",
            "Content classification training confusion matrix:  [[ 404   61   19   14]\n",
            " [  63  514  116   17]\n",
            " [  28   72  360   57]\n",
            " [  45   58  176 1772]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6112, device='cuda:0')\n",
            "verification training accuracy:  0.7942266949152542\n",
            "verification validation accuracy:  0.7801932367149759\n",
            "verification training confusion matrix:  [[2187  288]\n",
            " [ 489  812]]\n",
            "Content classification training accuracy:  0.8098516949152542\n",
            "Content classification validation accuracy:  0.7971014492753623\n",
            "Content classification training confusion matrix:  [[ 394   54   32   18]\n",
            " [  68  522  105   15]\n",
            " [  38   50  378   51]\n",
            " [  47   63  177 1764]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.5422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6151, device='cuda:0')\n",
            "verification training accuracy:  0.7939618644067796\n",
            "verification validation accuracy:  0.7801932367149759\n",
            "verification training confusion matrix:  [[2177  298]\n",
            " [ 480  821]]\n",
            "Content classification training accuracy:  0.8164724576271186\n",
            "Content classification validation accuracy:  0.7801932367149759\n",
            "Content classification training confusion matrix:  [[ 398   54   32   14]\n",
            " [  67  529  100   14]\n",
            " [  34   56  370   57]\n",
            " [  49   62  154 1786]]\n",
            "validation started.. 414\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6121, device='cuda:0')\n",
            "verification training accuracy:  0.8013771186440678\n",
            "verification validation accuracy:  0.7874396135265701\n",
            "verification training confusion matrix:  [[2206  269]\n",
            " [ 481  820]]\n",
            "Content classification training accuracy:  0.8241525423728814\n",
            "Content classification validation accuracy:  0.7463768115942029\n",
            "Content classification training confusion matrix:  [[ 411   49   26   12]\n",
            " [  63  536   95   16]\n",
            " [  31   65  368   53]\n",
            " [  49   66  139 1797]]\n",
            "validation started.. 414\n",
            "Epoch    10: reducing learning rate of group 0 to 1.0000e-06.\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.5318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6126, device='cuda:0')\n",
            "verification training accuracy:  0.8021716101694916\n",
            "verification validation accuracy:  0.7874396135265701\n",
            "verification training confusion matrix:  [[2206  269]\n",
            " [ 478  823]]\n",
            "Content classification training accuracy:  0.8246822033898306\n",
            "Content classification validation accuracy:  0.7584541062801933\n",
            "Content classification training confusion matrix:  [[ 410   50   26   12]\n",
            " [  66  531   96   17]\n",
            " [  35   51  373   58]\n",
            " [  47   56  148 1800]]\n",
            "Validation accuracy of the model is  0.788647342995169\n",
            "Now Testing: germanwings-crash.txt\n",
            "=================Testing===================\n",
            "germanwings-crash.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.68116   0.40693   0.50949       231\n",
            "         1.0    0.58610   0.81513   0.68190       238\n",
            "\n",
            "    accuracy                        0.61407       469\n",
            "   macro avg    0.63363   0.61103   0.59569       469\n",
            "weighted avg    0.63292   0.61407   0.59698       469\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.43885   0.79221   0.56481        77\n",
            "           1    0.61765   0.30657   0.40976       137\n",
            "           2    0.39881   0.59292   0.47687       113\n",
            "           3    0.89362   0.59155   0.71186       142\n",
            "\n",
            "    accuracy                        0.54158       469\n",
            "   macro avg    0.58723   0.57081   0.54083       469\n",
            "weighted avg    0.61912   0.54158   0.54285       469\n",
            "\n",
            "Validation accuracy of the model is  0.7729468599033817\n",
            "Iteration  10  Loss:  tensor(125.5085, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n",
            "model intialising...\n",
            "Training Set: {'charliehebdo.txt', 'germanwings-crash.txt', 'ottawashooting.txt'}\n",
            "1221\n",
            "size of training data 3100\n",
            "training started....\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  0\n",
            "Training Loss:  tensor(0.9692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.7836, device='cuda:0')\n",
            "verification training accuracy:  0.6341935483870967\n",
            "verification validation accuracy:  0.7130177514792899\n",
            "verification training confusion matrix:  [[1811  242]\n",
            " [ 892  155]]\n",
            "Content classification training accuracy:  0.5045161290322581\n",
            "Content classification validation accuracy:  0.7248520710059172\n",
            "Content classification training confusion matrix:  [[ 208   89   81   52]\n",
            " [ 191  231  148   55]\n",
            " [  83  105  113  114]\n",
            " [  77   71  470 1012]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  1\n",
            "Training Loss:  tensor(0.7209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6276, device='cuda:0')\n",
            "verification training accuracy:  0.7603225806451613\n",
            "verification validation accuracy:  0.7751479289940828\n",
            "verification training confusion matrix:  [[1957   96]\n",
            " [ 647  400]]\n",
            "Content classification training accuracy:  0.733225806451613\n",
            "Content classification validation accuracy:  0.7662721893491125\n",
            "Content classification training confusion matrix:  [[ 311   76   30   13]\n",
            " [ 126  352  130   17]\n",
            " [  37   80  221   77]\n",
            " [  27   64  150 1389]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  2\n",
            "Training Loss:  tensor(0.6342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6012, device='cuda:0')\n",
            "verification training accuracy:  0.7841935483870968\n",
            "verification validation accuracy:  0.757396449704142\n",
            "verification training confusion matrix:  [[1894  159]\n",
            " [ 510  537]]\n",
            "Content classification training accuracy:  0.772258064516129\n",
            "Content classification validation accuracy:  0.7810650887573964\n",
            "Content classification training confusion matrix:  [[ 339   46   31   14]\n",
            " [  91  406  121    7]\n",
            " [  32   62  266   55]\n",
            " [  32   59  156 1383]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  3\n",
            "Training Loss:  tensor(0.5859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5926, device='cuda:0')\n",
            "verification training accuracy:  0.8012903225806451\n",
            "verification validation accuracy:  0.7751479289940828\n",
            "verification training confusion matrix:  [[1882  171]\n",
            " [ 445  602]]\n",
            "Content classification training accuracy:  0.7925806451612903\n",
            "Content classification validation accuracy:  0.772189349112426\n",
            "Content classification training confusion matrix:  [[ 350   42   26   12]\n",
            " [  59  449  109    8]\n",
            " [  28   60  276   51]\n",
            " [  29   64  155 1382]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  4\n",
            "Training Loss:  tensor(0.5609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5856, device='cuda:0')\n",
            "verification training accuracy:  0.8045161290322581\n",
            "verification validation accuracy:  0.7751479289940828\n",
            "verification training confusion matrix:  [[1861  192]\n",
            " [ 414  633]]\n",
            "Content classification training accuracy:  0.8041935483870968\n",
            "Content classification validation accuracy:  0.7899408284023669\n",
            "Content classification training confusion matrix:  [[ 355   38   23   14]\n",
            " [  58  458  101    8]\n",
            " [  25   63  277   50]\n",
            " [  32   55  140 1403]]\n",
            "Validation accuracy of the model is  0.7825443786982249\n",
            "Now Testing: sydneysiege.txt\n",
            "=================Testing===================\n",
            "sydneysiege.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.63671   0.94278   0.76009       699\n",
            "         1.0    0.78495   0.27969   0.41243       522\n",
            "\n",
            "    accuracy                        0.65930      1221\n",
            "   macro avg    0.71083   0.61123   0.58626      1221\n",
            "weighted avg    0.70009   0.65930   0.61146      1221\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.72000   0.23529   0.35468       153\n",
            "           1    0.46127   0.56466   0.50775       232\n",
            "           2    0.45455   0.59735   0.51625       226\n",
            "           3    0.89153   0.86230   0.87667       610\n",
            "\n",
            "    accuracy                        0.67813      1221\n",
            "   macro avg    0.63183   0.56490   0.56384      1221\n",
            "weighted avg    0.70740   0.67813   0.67445      1221\n",
            "\n",
            "Validation accuracy of the model is  0.7825443786982249\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  5\n",
            "Training Loss:  tensor(0.5352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5500, device='cuda:0')\n",
            "verification training accuracy:  0.8096774193548387\n",
            "verification validation accuracy:  0.7928994082840237\n",
            "verification training confusion matrix:  [[1856  197]\n",
            " [ 393  654]]\n",
            "Content classification training accuracy:  0.8106451612903226\n",
            "Content classification validation accuracy:  0.8106508875739645\n",
            "Content classification training confusion matrix:  [[ 352   40   25   13]\n",
            " [  59  476   79   11]\n",
            " [  27   52  287   49]\n",
            " [  25   63  144 1398]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  6\n",
            "Training Loss:  tensor(0.5188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5414, device='cuda:0')\n",
            "verification training accuracy:  0.8190322580645162\n",
            "verification validation accuracy:  0.8136094674556213\n",
            "verification training confusion matrix:  [[1875  178]\n",
            " [ 383  664]]\n",
            "Content classification training accuracy:  0.827741935483871\n",
            "Content classification validation accuracy:  0.7928994082840237\n",
            "Content classification training confusion matrix:  [[ 353   46   21   10]\n",
            " [  49  488   77   11]\n",
            " [  22   52  301   40]\n",
            " [  32   50  124 1424]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  7\n",
            "Training Loss:  tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5688, device='cuda:0')\n",
            "verification training accuracy:  0.8193548387096774\n",
            "verification validation accuracy:  0.8076923076923077\n",
            "verification training confusion matrix:  [[1848  205]\n",
            " [ 355  692]]\n",
            "Content classification training accuracy:  0.8254838709677419\n",
            "Content classification validation accuracy:  0.7928994082840237\n",
            "Content classification training confusion matrix:  [[ 356   45   19   10]\n",
            " [  54  490   71   10]\n",
            " [  21   47  308   39]\n",
            " [  32   60  133 1405]]\n",
            "validation started.. 338\n",
            "errors  0\n",
            "Iteration  8\n",
            "Training Loss:  tensor(0.5233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.5559, device='cuda:0')\n",
            "verification training accuracy:  0.8212903225806452\n",
            "verification validation accuracy:  0.8106508875739645\n",
            "verification training confusion matrix:  [[1853  200]\n",
            " [ 354  693]]\n",
            "Content classification training accuracy:  0.822258064516129\n",
            "Content classification validation accuracy:  0.7810650887573964\n",
            "Content classification training confusion matrix:  [[ 356   40   25    9]\n",
            " [  48  499   66   12]\n",
            " [  27   53  297   38]\n",
            " [  32   54  147 1397]]\n",
            "validation started.. 338\n",
            "Epoch    10: reducing learning rate of group 0 to 1.0000e-06.\n",
            "errors  0\n",
            "Iteration  9\n",
            "Training Loss:  tensor(0.5187, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Validation loss:  tensor(0.6274, device='cuda:0')\n",
            "verification training accuracy:  0.8193548387096774\n",
            "verification validation accuracy:  0.7899408284023669\n",
            "verification training confusion matrix:  [[1856  197]\n",
            " [ 363  684]]\n",
            "Content classification training accuracy:  0.8229032258064516\n",
            "Content classification validation accuracy:  0.7869822485207101\n",
            "Content classification training confusion matrix:  [[ 349   41   25   15]\n",
            " [  50  505   61    9]\n",
            " [  30   48  301   36]\n",
            " [  37   55  142 1396]]\n",
            "Validation accuracy of the model is  0.8032544378698225\n",
            "Now Testing: sydneysiege.txt\n",
            "=================Testing===================\n",
            "sydneysiege.txt\n",
            "Verification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.74293   0.86409   0.79894       699\n",
            "         1.0    0.76716   0.59962   0.67312       522\n",
            "\n",
            "    accuracy                        0.75102      1221\n",
            "   macro avg    0.75504   0.73185   0.73603      1221\n",
            "weighted avg    0.75329   0.75102   0.74515      1221\n",
            "\n",
            "Content Classification\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.57480   0.47712   0.52143       153\n",
            "           1    0.45690   0.68534   0.54828       232\n",
            "           2    0.47009   0.48673   0.47826       226\n",
            "           3    0.92969   0.78033   0.84848       610\n",
            "\n",
            "    accuracy                        0.66994      1221\n",
            "   macro avg    0.60787   0.60738   0.59911      1221\n",
            "weighted avg    0.71031   0.66994   0.68193      1221\n",
            "\n",
            "Validation accuracy of the model is  0.7884615384615385\n",
            "Iteration  10  Loss:  tensor(100.6360, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Training and Testing Complete\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}