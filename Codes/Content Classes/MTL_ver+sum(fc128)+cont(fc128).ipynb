{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL-ver+sum(fc128)+cont(fc128).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba16dd38880e4542841a35b71748445a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ebc0924598a14e6e8dbf5e7aa651d7a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b11531945434ea3b7d37ea1c759c6b2",
              "IPY_MODEL_fa22966a5de94992931748ac1207a8ae"
            ]
          }
        },
        "ebc0924598a14e6e8dbf5e7aa651d7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b11531945434ea3b7d37ea1c759c6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53823db981354de3ac02c52c0e4f126a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_65eff835a61e4e85b8e709de5834d7ba"
          }
        },
        "fa22966a5de94992931748ac1207a8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2b182c9154824af0921f5b33d303f1ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 13.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70a23656d9b143f6a0774662e6d2280f"
          }
        },
        "53823db981354de3ac02c52c0e4f126a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "65eff835a61e4e85b8e709de5834d7ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b182c9154824af0921f5b33d303f1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70a23656d9b143f6a0774662e6d2280f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08aa0a064c18467aaa3216b45da6b3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dfc8ad5d80ec472fb88a3be7eb1c1d0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93f18ff4b5144e05948791cf78626eb0",
              "IPY_MODEL_cc2236ffcb0b48c1b39fad0ff57e5429"
            ]
          }
        },
        "dfc8ad5d80ec472fb88a3be7eb1c1d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93f18ff4b5144e05948791cf78626eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f3da8fdbd6bc468f896d3d92a7eefa14",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_532473790e8743759f40a5018cfd9f78"
          }
        },
        "cc2236ffcb0b48c1b39fad0ff57e5429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74b4d14df6584afd9fe7cf64de724e7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:09&lt;00:00, 47.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cadedfd209b4afeb0339f91905b81d7"
          }
        },
        "f3da8fdbd6bc468f896d3d92a7eefa14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "532473790e8743759f40a5018cfd9f78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74b4d14df6584afd9fe7cf64de724e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cadedfd209b4afeb0339f91905b81d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0lpKalclMI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b08534a4-84bf-4eb4-987e-c61ce667d679"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3qDIKA0lPIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "83687763-f857-442b-a1b9-09d6fb564d92"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 12.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=570f938d63bdc095981ef5e23a37e9471c0d151a061ceef77ab2382023389ba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYJIQAUciVGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import random\n",
        "import numpy\n",
        "import itertools\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImOTKZTxi2AJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "90a734a0-7c3f-40e9-ff15-ddcca4e12b87"
      },
      "source": [
        "# # If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\t# Tell PyTorch to use the GPU.    \n",
        "\tdevice = torch.device(\"cuda\")\n",
        "\tprint('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\tprint('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "\tprint('No GPU available, using the CPU instead.')\n",
        "\tdevice = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy37-fOTixXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "9f7f1221-662e-4bf5-ed3f-270bdd4aa326"
      },
      "source": [
        "MODEL_NAME = 'BERT'\n",
        "print(f'MODEL_NAME = {MODEL_NAME}')\n",
        "MODEL_SAVING_POLICY = \"acc\" # 'loss'\n",
        "print(f'MODEL_SAVING_POLICY = {MODEL_SAVING_POLICY}')\n",
        "LOSS_FN = 'nw' # 'w'\n",
        "print(f'LOSS_FN = {LOSS_FN}')\n",
        "OPTIM = 'adam' # 'adamw'\n",
        "print(f'OPTIM = {OPTIM}')\n",
        "L2_REGULARIZER = 'n' # 'y'\n",
        "print(f'L2_REGULARIZER = {L2_REGULARIZER}')\n",
        "USE_DROPOUT = 'n' # 'y'\n",
        "print(f'USE_DROPOUT = {USE_DROPOUT}')\n",
        "TREE_VERSION = \"old\" # 'new'\n",
        "print(f'TREE_VERSION = {TREE_VERSION}')\n",
        "NO_OF_EVENTS = 4\n",
        "print(f'NO_OF_EVENTS = {NO_OF_EVENTS}')\n",
        "gpu_id = 0\n",
        "print(f'GPU_ID = {gpu_id}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL_NAME = BERT\n",
            "MODEL_SAVING_POLICY = acc\n",
            "LOSS_FN = nw\n",
            "OPTIM = adam\n",
            "L2_REGULARIZER = n\n",
            "USE_DROPOUT = n\n",
            "TREE_VERSION = old\n",
            "NO_OF_EVENTS = 4\n",
            "GPU_ID = 0\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnbRUQKJi9jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, data):\n",
        "\t\tself.data = data\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\t\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.data[idx]\n",
        "\n",
        "\n",
        "def _label_node_index(node, n=0):\n",
        "\tnode['index'] = n\n",
        "\tfor child in node['c']:\n",
        "\t\tn += 1\n",
        "\t\t_label_node_index(child, n)\n",
        "\n",
        "\n",
        "def _gather_node_attributes(node, key):\n",
        "\tfeatures = [node[key]]\n",
        "\tfor child in node['c']:\n",
        "\t\tfeatures.extend(_gather_node_attributes(child, key))\n",
        "\treturn features\n",
        "\n",
        "\n",
        "def _gather_adjacency_list(node):\n",
        "\tadjacency_list = []\n",
        "\tfor child in node['c']:\n",
        "\t\tadjacency_list.append([node['index'], child['index']])\n",
        "\t\tadjacency_list.extend(_gather_adjacency_list(child))\n",
        "\n",
        "\treturn adjacency_list\n",
        "\n",
        "\n",
        "def convert_tree_to_tensors(tree, summ_gt, cont_gt, tweet_id, device=device):\n",
        "\t# Label each node with its walk order to match nodes to feature tensor indexes\n",
        "\t# This modifies the original tree as a side effect\n",
        "\t_label_node_index(tree)\n",
        "\n",
        "\tfeatures = _gather_node_attributes(tree, 'f')\n",
        "\tattention = _gather_node_attributes(tree, 'a')\n",
        "\told_features = _gather_node_attributes(tree, 'k')\n",
        "\tlabels = _gather_node_attributes(tree, 'l')\t\t\n",
        "\troot_label = [labels[0]]\n",
        "\tif summ_gt==1:\n",
        "\t\ts_gt = [[0,1]]\n",
        "\telse:\n",
        "\t\ts_gt = [[1,0]]\n",
        "\tc_gt = [cont_gt]\n",
        "\tadjacency_list = _gather_adjacency_list(tree)\n",
        "\n",
        "\tnode_order, edge_order = calculate_evaluation_orders(adjacency_list, len(features))\n",
        "\troot_node = [0]\n",
        "\n",
        "\treturn {\n",
        "\t\t'f': torch.tensor(features, dtype=torch.long),\n",
        "\t\t'a':torch.tensor(attention,  dtype=torch.float32),\n",
        "\t\t'k':torch.tensor(old_features, dtype=torch.float32),\n",
        "        's_gt':torch.tensor(s_gt,dtype=torch.float32),\n",
        "        'c_gt':torch.tensor(c_gt,dtype=torch.long),\n",
        "\t\t'l': torch.tensor(labels,  dtype=torch.float32),\n",
        "\t\t'root_l': torch.tensor(root_label, dtype=torch.long),\n",
        "\t\t'root_n': torch.tensor(root_node,  dtype=torch.int64),\n",
        "\t\t'node_order': torch.tensor(node_order,  dtype=torch.int64),\n",
        "\t\t'adjacency_list': torch.tensor(adjacency_list,  dtype=torch.int64),\n",
        "\t\t'edge_order': torch.tensor(edge_order,  dtype=torch.int64),\n",
        "        'tweet_id' : torch.tensor(tweet_id, dtype=torch.int64)\n",
        "\t}\n",
        "\n",
        "\n",
        "def calculate_evaluation_orders(adjacency_list, tree_size):\n",
        "\t'''Calculates the node_order and edge_order from a tree adjacency_list and the tree_size.\n",
        "\n",
        "\tThe TreeLSTM model requires node_order and edge_order to be passed into the model along\n",
        "\twith the node features and adjacency_list.  We pre-calculate these orders as a speed\n",
        "\toptimization.\n",
        "\t'''\n",
        "\tadjacency_list = numpy.array(adjacency_list)\n",
        "\tnode_ids = numpy.arange(tree_size, dtype=int)\n",
        "\tnode_order = numpy.zeros(tree_size, dtype=int)\n",
        "\tunevaluated_nodes = numpy.ones(tree_size, dtype=bool)\n",
        "\t\n",
        "\t# print(adjacency_list)\n",
        "\tif(len(adjacency_list)==0):\n",
        "\t\treturn [0],[]\n",
        "\tparent_nodes = adjacency_list[:, 0]\n",
        "\tchild_nodes = adjacency_list[:, 1]\n",
        "\n",
        "\tn = 0\n",
        "\twhile unevaluated_nodes.any():\n",
        "\t\t# Find which child nodes have not been evaluated\n",
        "\t\tunevaluated_mask = unevaluated_nodes[child_nodes]\n",
        "\n",
        "\t\t# Find the parent nodes of unevaluated children\n",
        "\t\tunready_parents = parent_nodes[unevaluated_mask]\n",
        "\n",
        "\t\t# Mark nodes that have not yet been evaluated\n",
        "\t\t# and which are not in the list of parents with unevaluated child nodes\n",
        "\t\tnodes_to_evaluate = unevaluated_nodes & ~numpy.isin(node_ids, unready_parents)\n",
        "\n",
        "\t\tnode_order[nodes_to_evaluate] = n\n",
        "\t\tunevaluated_nodes[nodes_to_evaluate] = False\n",
        "\n",
        "\t\tn += 1\n",
        "\n",
        "\tedge_order = node_order[parent_nodes]\n",
        "\n",
        "\treturn node_order, edge_order\n",
        "\n",
        "\n",
        "def batch_tree_input(batch):\n",
        "\t'''Combines a batch of tree dictionaries into a single batched dictionary for use by the TreeLSTM model.\n",
        "\n",
        "\tbatch - list of dicts with keys ('f', 'node_order', 'edge_order', 'adjacency_list')\n",
        "\treturns a dict with keys ('f', 'node_order', 'edge_order', 'adjacency_list', 'tree_sizes')\n",
        "\t'''\n",
        "\ttree_sizes = [b['f'].shape[0] for b in batch]\n",
        "\n",
        "\tbatched_features = torch.cat([b['f'] for b in batch])\n",
        "\tbatched_attentions = torch.cat([b['a'] for b in batch])\n",
        "\tbatched_old_features = torch.cat([b['k'] for b in batch])\n",
        "\tbatched_node_order = torch.cat([b['node_order'] for b in batch])\n",
        "\n",
        "\tidx = 0\n",
        "\troot_li = []\n",
        "\n",
        "\tfor b in batch:\n",
        "\t\troot_li.append(idx)\n",
        "\t\tidx += len(b['node_order'])\n",
        "\n",
        "\tbatched_root = torch.tensor(root_li, dtype=torch.int64)\n",
        "\n",
        "\tbatched_edge_order = torch.cat([b['edge_order'] for b in batch])\n",
        "\n",
        "\tbatched_labels = torch.cat([b['l'] for b in batch])\n",
        "\n",
        "\tbatched_root_labels = torch.cat([b['root_l'] for b in batch])\n",
        "\tbatched_summ_labels = torch.cat([b['s_gt'] for b in batch])\n",
        " \n",
        "\tbatched_cont_labels = torch.cat([b['c_gt'] for b in batch])\n",
        "\tbatched_adjacency_list = []\n",
        "\toffset = 0\n",
        "\tfor n, b in zip(tree_sizes, batch):\n",
        "\t\tbatched_adjacency_list.append(b['adjacency_list'] + offset)\n",
        "\t\toffset += n\n",
        "\tbatched_adjacency_list = torch.cat(batched_adjacency_list)\n",
        "\n",
        "\treturn {\n",
        "\t\t'f': batched_features,\n",
        "\t\t'a': batched_attentions,\n",
        "\t\t'k': batched_old_features,\n",
        "        's_gt':batched_summ_labels,\n",
        "        'c_gt':batched_cont_labels,\n",
        "\t\t'node_order': batched_node_order,\n",
        "\t\t'edge_order': batched_edge_order,\n",
        "\t\t'adjacency_list': batched_adjacency_list,\n",
        "\t\t'tree_sizes': tree_sizes,\n",
        "\t\t'root_node': batched_root,\n",
        "\t\t'root_label': batched_root_labels,\n",
        "\t\t'l': batched_labels\n",
        "\t}\n",
        "\n",
        "\n",
        "def unbatch_tree_tensor(tensor, tree_sizes):\n",
        "\t'''Convenience functo to unbatch a batched tree tensor into individual tensors given an array of tree_sizes.\n",
        "\n",
        "\tsum(tree_sizes) must equal the size of tensor's zeroth dimension.\n",
        "\t'''\n",
        "\treturn torch.split(tensor, tree_sizes, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsRW0qWMi--M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeLSTM(torch.nn.Module):\n",
        "\t'''PyTorch TreeLSTM model that implements efficient batching.\n",
        "\t'''\n",
        "\tdef __init__(self, model_name, trainable_layers, in_features, out_features, classifier_dropout, mode='cls'):\n",
        "\t\t'''TreeLSTM class initializer\n",
        "\n",
        "\t\tTakes in int sizes of in_features and out_features and sets up model Linear network layers.\n",
        "\t\t'''\n",
        "\t\tsuper().__init__()\n",
        "\t\tprint(\"model intialising...\")\n",
        "\t\tself.in_features = in_features\n",
        "\t\tself.out_features = out_features\n",
        "\t\tself.mode = mode\n",
        "\t\tself.model_name = model_name\n",
        "\t\t\n",
        "        #ENCODER\n",
        "\t\tif model_name == 'BERT':\n",
        "\t\t\tself.BERT_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\t\telif model_name == 'ROBERTA':\n",
        "\t\t\tself.BERT_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\t\telif model_name == 'XLNET':\n",
        "\t\t\tself.BERT_model = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
        "\t\telif model_name == 'T5':\n",
        "\t\t\tself.BERT_model = T5Model.from_pretrained(\"t5-base\")\n",
        "\t\telse:\n",
        "\t\t\t# Default BERT\n",
        "\t\t\tself.BERT_model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\t\t\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tfor name, param in self.BERT_model.named_parameters():\n",
        "\t\t\tflag = False\n",
        "\t\t\tfor num in trainable_layers:\n",
        "\t\t\t\tif 'layer.'+ str(num) + '.' in name:\n",
        "\t\t\t\t\tparam.requires_grad = True\n",
        "\t\t\t\t\tflag = True\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\tif not flag:\n",
        "\t\t\t\tif 'pooler' in name or 'embedding' in name:\n",
        "\t\t\t\t\tparam.requires_grad = True\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tparam.requires_grad = False\n",
        "        \"\"\"\n",
        "\n",
        "        #DECODER\n",
        "        #VERIFICATION\n",
        "\t\tself.W_iou = torch.nn.Linear(self.in_features, 3 * self.out_features)\n",
        "\t\tself.U_iou = torch.nn.Linear(self.out_features, 3 * self.out_features, bias=False)\n",
        "\t\t# f terms are maintained seperate from the iou terms because they involve sums over child nodes\n",
        "\t\t# while the iou terms do not\n",
        "\t\tself.W_f = torch.nn.Linear(self.in_features, self.out_features)\n",
        "\t\tself.U_f = torch.nn.Linear(self.out_features, self.out_features, bias=False)\n",
        "\t\tself.fc = torch.nn.Linear(self.out_features, 1)\n",
        "\t\t# self.bert_dropout = torch.nn.Dropout(bert_dropout)\n",
        "\t\tself.classifier_dropout = torch.nn.Dropout(classifier_dropout)\n",
        "\t\t# self.init_weights()\n",
        "\n",
        "        #SUMMARIZATION\n",
        "\t\tself.summ_fc1 = torch.nn.Linear(self.in_features,self.out_features)\n",
        "\t\tself.summ_fc2 = torch.nn.Linear(self.out_features,2)\n",
        "\n",
        "        #CONTENT-CLASSIFICATION\n",
        "\t\tself.cont_fc1 = torch.nn.Linear(self.in_features,self.out_features)\n",
        "\t\tself.cont_fc2 = torch.nn.Linear(self.out_features,4)\n",
        "\t\n",
        "\tdef init_weights(self):\n",
        "\t\tfor name, param in self.named_parameters():\n",
        "\t\t\tif \"BERT\" in name or \"bias\" in name:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telse:\n",
        "\t\t\t\ttorch.nn.init.xavier_uniform_(param)\n",
        "\n",
        "\n",
        "\tdef forward(self, features, attentions, old_features, node_order, adjacency_list, edge_order, root_node):\n",
        "\t\t'''Run TreeLSTM model on a tree data structure with node features\n",
        "\n",
        "\t\tTakes Tensors encoding node features, a tree node adjacency_list, and the order in which \n",
        "\t\tthe tree processing should proceed in node_order and edge_order.\n",
        "\t\t'''\n",
        "\n",
        "\t\t# Total number of nodes in every tree in the batch\n",
        "\t\tbatch_size = node_order.shape[0]\n",
        "\n",
        "\t\t# Retrive device the model is currently loaded on to generate h, c, and h_sum result buffers\n",
        "\t\tdevice = next(self.parameters()).device\n",
        "\n",
        "\t\t# h and c states for every node in the batch\n",
        "\t\t# h - hidden state\n",
        "\t\t# c - memory state\n",
        "\t\th = torch.zeros(batch_size, self.out_features, device=device)\t\t\n",
        "\t\tc = torch.zeros(batch_size, self.out_features, device=device)\n",
        "\t\t\n",
        "\t\tif self.model_name == 'XLNET':\n",
        "\t\t\thidden_states = self.BERT_model(input_ids=features, attention_mask=attentions)\n",
        "\t\t\thidden_states = hidden_states[0]\n",
        "\t\t\t# print(len(hidden_states))\n",
        "\t\t\tprint(hidden_states[0])\n",
        "\t\t\t# print(hidden_states[0].size)\n",
        "\t\telse:\n",
        "\t\t\thidden_states,_ = self.BERT_model(input_ids=features, attention_mask=attentions)\t\t\n",
        "\n",
        "\t\tif self.mode==\"cls\":\n",
        "\t\t\toutput_vectors = hidden_states[:,0]\n",
        "\t\telif self.mode==\"avg\":\n",
        "\t\t\tinput_mask_expanded = attentions.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "\t\t\tsum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
        "\t\t\tsum_mask = input_mask_expanded.sum(1)\n",
        "\t\t\toutput_vectors= sum_embeddings / sum_mask\n",
        "\t\t\t\n",
        "\t\toutput_vectors = torch.cat([output_vectors, old_features], axis=1)\n",
        "\t\t# output_vectors = self.bert_dropout(output_vectors)\n",
        "\t\t\n",
        "        #DECODER\n",
        "        #VERIFICATION\n",
        "\t\tfor n in range(node_order.max() + 1):\n",
        "\t\t\tself._run_lstm(n, h, c, output_vectors, node_order, adjacency_list, edge_order)\n",
        "\t\th_root = h[root_node, :]\n",
        "\t\tif USE_DROPOUT == 'y':\n",
        "\t\t\th_root = self.classifier_dropout(h_root)\n",
        "\t\tlogits_out = self.fc(h_root)\n",
        "\t\t# pred_out = F.log_softmax(logits_out, dim = 1)\n",
        "\t\t# pred_out = F.softmax(logits_out, dim = 1)\n",
        "\n",
        "        #SUMMARIZAION\n",
        "\t\ts_x = output_vectors[root_node,:]\n",
        "\t\ts_x = self.summ_fc1(s_x)\n",
        "\t\ts_x = self.summ_fc2(s_x)\n",
        "\t\ts_x = torch.nn.functional.softmax(s_x,dim=1)\n",
        "  \n",
        "        #CONTENT-CLASSIFICATION\n",
        "\t\tc_x = output_vectors[root_node,:]\n",
        "\t\tc_x = self.cont_fc1(c_x)\n",
        "\t\tc_x = self.cont_fc2(c_x)\n",
        "\n",
        "\t\treturn h, logits_out, c, s_x, c_x\n",
        "\n",
        "\t\n",
        "\tdef _run_lstm(self, iteration, h, c, features, node_order, adjacency_list, edge_order):\n",
        "\t\t'''Helper function to evaluate all tree nodes currently able to be evaluated.\n",
        "\t\t'''\n",
        "\t\tnode_mask = node_order == iteration\n",
        "\n",
        "\t\t# edge_mask is a tensor of size E x 1\n",
        "\t\tedge_mask = edge_order == iteration\n",
        "\n",
        "\t\tx = features[node_mask, :]\n",
        "\t\tif iteration == 0:\n",
        "\t\t\tiou = self.W_iou(x)\n",
        "\t\telse:\n",
        "\t\t\t# adjacency_list is a tensor of size e x 2\n",
        "\t\t\tadjacency_list = adjacency_list[edge_mask, :]\n",
        "\n",
        "\t\t\tparent_indexes = adjacency_list[:, 0]\n",
        "\t\t\tchild_indexes = adjacency_list[:, 1]\n",
        "\n",
        "\t\t\t# child_h and child_c are tensors of size e x 1\n",
        "\t\t\tchild_h = h[child_indexes, :]\n",
        "\t\t\tchild_c = c[child_indexes, :]\n",
        "\n",
        "\t\t\t# Add child hidden states to parent offset locations\n",
        "\t\t\t_, child_counts = torch.unique_consecutive(parent_indexes, return_counts=True)\n",
        "\t\t\tchild_counts = tuple(child_counts)\n",
        "\t\t\tparent_children = torch.split(child_h, child_counts)\n",
        "\t\t\tparent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "\t\t\th_sum = torch.stack(parent_list)\n",
        "\t\t\tiou = self.W_iou(x) + self.U_iou(h_sum)\n",
        "\n",
        "\n",
        "\t\t# i, o and u are tensors of size n x M\n",
        "\t\ti, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n",
        "\t\ti = torch.sigmoid(i)\n",
        "\t\to = torch.sigmoid(o)\n",
        "\t\tu = torch.tanh(u)\n",
        "\n",
        "\t\tif iteration == 0:\n",
        "\t\t\tc[node_mask, :] = i * u\n",
        "\t\telse:\n",
        "\t\t\t# f is a tensor of size e x M\n",
        "\t\t\tf = self.W_f(features[parent_indexes, :]) + self.U_f(child_h)\n",
        "\t\t\tf = torch.sigmoid(f)\n",
        "\t\t\t# fc is a tensor of size e x M\n",
        "\t\t\tfc = f * child_c\n",
        "\t\t\t# Add the calculated f values to the parent's memory cell state\n",
        "\t\t\tparent_children = torch.split(fc, child_counts)\n",
        "\t\t\tparent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "\t\t\tc_sum = torch.stack(parent_list)\n",
        "\t\t\tc[node_mask, :] = i * u + c_sum\n",
        "\n",
        "\t\th[node_mask, :] = o * torch.tanh(c[node_mask])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jfUPXorjFRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, name, val_acc=0, val_loss=1):\n",
        "\tstate = {\n",
        "\t\t'model':model.state_dict(),\n",
        "\t\t'optimizer': optimizer.state_dict(),\n",
        "\t\t'val_acc': val_acc,\n",
        "\t\t'val_loss': val_loss\n",
        "\t\t}\n",
        "\ttorch.save(state, name)\n",
        "\n",
        "\n",
        "def load_model(model, name):\n",
        "\tstate = torch.load(name)\n",
        "\tmodel.load_state_dict(state['model'])\n",
        "\toptimizer.load_state_dict(state['optimizer'])\n",
        "\tprint('Validation accuracy of the model is ', state.get('val_acc'))\n",
        "\tprint('Validation loss of the model is ', state.get('val_loss'))\n",
        "\treturn state.get('val_acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLUKHQ6djIy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data_verification(trees, frac):\n",
        "\tpos_data = []\n",
        "\tneg_data = []\n",
        "\tfor tree in trees:\n",
        "\t\tif tree['root_l'].tolist() == [[0, 1]]:\n",
        "\t\t\tpos_data.append(tree)\n",
        "\t\telse:\n",
        "\t\t\tneg_data.append(tree)\n",
        "\tpos_len = int(frac * len(pos_data))\n",
        "\tneg_len = int(frac * len(neg_data))\n",
        "\tval_li = pos_data[:pos_len] + neg_data[:neg_len]\n",
        "\trandom.shuffle(val_li)\n",
        "\ttrain_li = pos_data[pos_len:] + neg_data[neg_len:]\n",
        "\trandom.shuffle(train_li)\n",
        "\treturn train_li, val_li"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFoR3XUvqoH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data_summary(trees,frac):\n",
        "    pos_data = []\n",
        "    neg_data = []\n",
        "    for tree in trees:\n",
        "        if tree['s_gt'].tolist() == [[0,1]]:\n",
        "            pos_data.append(tree)\n",
        "        else:\n",
        "            neg_data.append(tree)\n",
        "    pos_len = int(frac*len(pos_data))\n",
        "    neg_len = int(frac*len(neg_data))\n",
        "    val_li = pos_data[:pos_len]+neg_data[:neg_len]\n",
        "    random.shuffle(val_li)\n",
        "    train_li = pos_data[pos_len:]+neg_data[neg_len:]\n",
        "    random.shuffle(train_li)\n",
        "    return train_li,val_li"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9NJlk76tcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data_content(trees,frac):\n",
        "    data = [[] for _ in range(4)]\n",
        "    for tree in trees:\n",
        "        data[int(tree['c_gt'].item()[0])].append(tree)\n",
        "    length = [int(frac*len(data[i])) for i in range(4)]\n",
        "    print([len(data[i][:length[i]]) for i in range(4)])\n",
        "    \n",
        "    val_li = []\n",
        "    for i in range(4):\n",
        "        val_li.extend(data[i][:length[i]])\n",
        "    random.shuffle(val_li)\n",
        "    train_li = []\n",
        "    for i in range(4):\n",
        "        train_li.extend(data[i][length[i]:])\n",
        "    random.shuffle(train_li)\n",
        "\n",
        "    return train_li,val_li"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUuyIJR1jM_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"./drive/My Drive/\"\n",
        "name = path + \"mtl_ver+cont+summ.pt\"\n",
        "name2 = path + \"mtl_ver+cont+summ_2.pt\"\n",
        "if MODEL_NAME == 'BERT':\n",
        "\t# tree_path = './PT_FeatBERT40_maxR5/'\n",
        "\tif TREE_VERSION == \"new\":\n",
        "\t\ttree_path = './PT_PHEME5_FeatBERT40_Depth5_maxR5/'\n",
        "\telse:\n",
        "\t\ttree_path = './drive/My Drive/Parsed-Trees-Pad32_FeatBERT40_Depth5_maxR5/'\n",
        "elif MODEL_NAME == 'ROBERTA':\n",
        "\t# tree_path = './PT_FeatROBERTA40_maxR5/'\n",
        "\ttree_path = './PT_PHEME5_FeatROBERTA40_Depth5_maxR5/'\n",
        "elif MODEL_NAME == 'XLNET':\n",
        "\t# tree_path = './PT_FeatXLNET40_maxR5/'\n",
        "\ttree_path = './PT_PHEME5_FeatXLNET40_Depth5_maxR5/'\n",
        "elif MODEL_NAME =='T5':\n",
        "\t# tree_path = './PT_FeatT540_maxR5/'\n",
        "\ttree_path = './PT_PHEME5_FeatT540_Depth5_maxR5/'\n",
        "else:\n",
        "\t# Default BERT\n",
        "\t# tree_path = './PT_FeatBERT40_maxR5/'\n",
        "\ttree_path = './PT_PHEME5_FeatBERT40_Depth5_maxR5/'\n",
        "\n",
        "if NO_OF_EVENTS == 4:\n",
        "\tfiles = ['charliehebdo.txt', 'germanwings-crash.txt', 'ottawashooting.txt','sydneysiege.txt']\n",
        "else:\n",
        "\tfiles = ['charliehebdo.txt', 'ferguson.txt', 'germanwings-crash.txt', 'ottawashooting.txt','sydneysiege.txt']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVc5LjPxsnXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "f = ['charliehebdo', 'ottawashooting','germanwings','sydneysiege'] \n",
        "path2 = \"./drive/My Drive/CIKM_dataset/0.7/\"\n",
        "dfc = pd.read_pickle(path2+f[0]+\"_7.pkl\")\n",
        "dfo = pd.read_pickle(path2+f[1]+\"_7.pkl\")\n",
        "dfg = pd.read_pickle(path2+f[2]+\"_7.pkl\")\n",
        "dfs = pd.read_pickle(path2+f[3]+\"_7.pkl\")\n",
        "\n",
        "summ_dict = {}\n",
        "for df in [dfc,dfo,dfg,dfs]:\n",
        "    for ind in df.index:\n",
        "        summ_dict[df['tweetid'][ind]] = df['newgt'][ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0agCXHGMsqcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10422b6d-4f82-48c5-9fd2-c1a47e0b1a6d"
      },
      "source": [
        "count = 0\n",
        "for i in summ_dict:\n",
        "    if(summ_dict[i]==1):\n",
        "        count+=1\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmvpTCgz7PpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files = ['charliehebdo.txt', 'ottawashooting.txt','germanwings-crash.txt','sydneysiege.txt'] \n",
        "cont_tweets = {}\n",
        "for f in files:\n",
        "    f = codecs.open(path+\"situational_tweets/\"+f[:-4]+\"_FOUR_CLEAN_ANNOTATE_110520.txt\")\n",
        "    for line in f:\n",
        "        line = line.split(\"\\t\")\n",
        "        cont_tweets[int(line[1])] = int(line[8])-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkVh5D-gjTNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1f104c69-3177-4bee-c728-95953d2dc1ba"
      },
      "source": [
        "tree_li = {}\n",
        "val_li = {}\n",
        "s_y = {}\n",
        "c_y = {}\n",
        "for filename in files:\n",
        "    s_temp = []\n",
        "    c_temp = []\n",
        "    input_file = codecs.open(tree_path + filename, 'r', 'utf-8')\n",
        "    tree_li[filename] = []\n",
        "    for row in input_file:\n",
        "        s = row.strip().split('\\t')\n",
        "        tweet_id = int(s[0])\n",
        "        curr_tree = eval(s[1])\n",
        "        curr_tensor = convert_tree_to_tensors(curr_tree,summ_dict[tweet_id],cont_tweets[tweet_id],tweet_id)\n",
        "        s_temp.append(summ_dict[tweet_id])\n",
        "        c_temp.append(cont_tweets[tweet_id])\n",
        "        tree_li[filename].append(curr_tensor)\n",
        "    s_y[filename] = s_temp\n",
        "    c_y[filename] = c_temp\n",
        "    random.shuffle(tree_li[filename])\n",
        "    tree_li[filename], val_li[filename] = split_data_summary(tree_li[filename], 0.2)\n",
        "    input_file.close()\n",
        "    print(f'{filename} Training Set Size: {len(tree_li[filename])}, Validation Set Size: {len(val_li[filename])}, Total: {len(tree_li[filename]) + len(val_li[filename])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charliehebdo.txt Training Set Size: 1664, Validation Set Size: 415, Total: 2079\n",
            "germanwings-crash.txt Training Set Size: 376, Validation Set Size: 93, Total: 469\n",
            "ottawashooting.txt Training Set Size: 712, Validation Set Size: 178, Total: 890\n",
            "sydneysiege.txt Training Set Size: 978, Validation Set Size: 243, Total: 1221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCsTgqFjjZNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "84eaa386-dd12-48b4-bc25-4354a872cc7f"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "weight_vec = {}\n",
        "pos_weight_vec = {}\n",
        "for test_file in files:\n",
        "\ty = []\n",
        "\tlabel_dist = [0, 0]\n",
        "\tfor filename in files:\t\t\n",
        "\t\tif filename != test_file:\t\t\t\n",
        "\t\t\tfile_dist = [0, 0]\n",
        "\t\t\tfor tree in tree_li[filename]:\n",
        "\t\t\t\t# print(int(tree['root_l'].tolist()[0][1]))\n",
        "\t\t\t\ty.append(int(tree['root_l'].tolist()[0][1]))\n",
        "\t\t\t\tfile_dist[int(tree['root_l'].tolist()[0][1])] += 1\n",
        "\t\t\t\tlabel_dist[int(tree['root_l'].tolist()[0][1])] += 1\n",
        "\t\t\t# print(f'{filename} has {file_dist[0]} non-rumors and {file_dist[1]} rumors')\n",
        "\tprint(f'Total non-rumors: {label_dist[0]}, Total rumors: {label_dist[1]}')\n",
        "\tweight_vec[test_file] = torch.tensor(compute_class_weight('balanced', numpy.unique(y), y)).to(device)\n",
        "\tpos_weight = label_dist[0] / label_dist[1]\n",
        "\tpos_weight_vec[test_file] = torch.tensor([pos_weight], dtype=torch.float32).to(device)\n",
        "\tprint(f'Test File: {test_file}, Weight Vector: {weight_vec[test_file]}')\n",
        "\tprint(f'Test File: {test_file}, Pos Weight Vector: {pos_weight_vec[test_file]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total non-rumors: 1077, Total rumors: 989\n",
            "Test File: charliehebdo.txt, Weight Vector: tensor([0.9591, 1.0445], device='cuda:0', dtype=torch.float64)\n",
            "Test File: charliehebdo.txt, Pos Weight Vector: tensor([1.0890], device='cuda:0')\n",
            "Total non-rumors: 2197, Total rumors: 1157\n",
            "Test File: germanwings-crash.txt, Weight Vector: tensor([0.7633, 1.4494], device='cuda:0', dtype=torch.float64)\n",
            "Test File: germanwings-crash.txt, Pos Weight Vector: tensor([1.8989], device='cuda:0')\n",
            "Total non-rumors: 2044, Total rumors: 974\n",
            "Test File: ottawashooting.txt, Weight Vector: tensor([0.7383, 1.5493], device='cuda:0', dtype=torch.float64)\n",
            "Test File: ottawashooting.txt, Pos Weight Vector: tensor([2.0986], device='cuda:0')\n",
            "Total non-rumors: 1819, Total rumors: 933\n",
            "Test File: sydneysiege.txt, Weight Vector: tensor([0.7565, 1.4748], device='cuda:0', dtype=torch.float64)\n",
            "Test File: sydneysiege.txt, Pos Weight Vector: tensor([1.9496], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAA6gXom7uKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "8a855b46-8b99-41de-9c22-219e4d890b27"
      },
      "source": [
        "content_weight_vec = {}\n",
        "summ_weight_vec = {}\n",
        "for test_file in files:\n",
        "    s_y = []\n",
        "    c_y = []\n",
        "    for f in files:\n",
        "        if f != test_file:\n",
        "            for t in tree_li[f]:\n",
        "                s_y.append(t['s_gt'][0][1].item())\n",
        "                c_y.append(t['c_gt'][0].item())\n",
        "    content_weight_vec[test_file] = torch.tensor(compute_class_weight('balanced',numpy.unique(c_y),c_y), device=device, dtype=torch.float32)\n",
        "    summ_weight_vec[test_file] = torch.tensor(compute_class_weight('balanced',numpy.unique(s_y),s_y),device=device, dtype=torch.float32)\n",
        "print(\"*******Content Weights******\")\n",
        "print(content_weight_vec)\n",
        "print(\"******Summary Weights******\")\n",
        "print(summ_weight_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*******Content Weights******\n",
            "{'charliehebdo.txt': tensor([1.6191, 1.0738, 1.2598, 0.6034], device='cuda:0'), 'germanwings-crash.txt': tensor([1.9637, 1.2724, 1.8032, 0.4651], device='cuda:0'), 'ottawashooting.txt': tensor([2.1870, 1.2111, 1.8225, 0.4612], device='cuda:0'), 'sydneysiege.txt': tensor([1.8595, 1.1924, 1.8445, 0.4804], device='cuda:0')}\n",
            "******Summary Weights******\n",
            "{'charliehebdo.txt': tensor([0.5826, 3.5256], device='cuda:0'), 'germanwings-crash.txt': tensor([0.5989, 3.0271], device='cuda:0'), 'ottawashooting.txt': tensor([0.6119, 2.7337], device='cuda:0'), 'sydneysiege.txt': tensor([0.6201, 2.5816], device='cuda:0')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeaDtXKPjoy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(tree_batch, test_file, mode=\"train\"):\n",
        "\terr_count = 0\n",
        "\tloss = 0\n",
        "\tpred_labels = []\n",
        "\tg_labels = []\n",
        "\t\n",
        "\t# try:\n",
        "\th, h_root, c, summ_out, cont_out = model(\n",
        "\t\ttree_batch['f'].to(device),\n",
        "\t\ttree_batch['a'].to(device),\n",
        "\t\ttree_batch['k'].to(device),\n",
        "\t\ttree_batch['node_order'].to(device),\n",
        "\t\ttree_batch['adjacency_list'].to(device),\n",
        "\t\ttree_batch['edge_order'].to(device),\n",
        "\t\ttree_batch['root_node'].to(device)\n",
        "\t)\n",
        "\n",
        "\t#WEIGHTS\n",
        "\tweights = weight_vec[test_file]\n",
        "\tpos_weights = pos_weight_vec[test_file]\n",
        "\tsumm_weight = summ_weight_vec[test_file]\n",
        "\tcont_weight = content_weight_vec[test_file]\n",
        "\n",
        "\n",
        "\t# CASE 1: Verification\n",
        "\troot = tree_batch['root_label'].to('cpu')\n",
        "\tg_labels = [[t[1]] for t in root]\n",
        "\tg_labels_tensor = torch.tensor(g_labels).type_as(h_root).to(device)\n",
        "\tpred_logits = h_root.detach().cpu()\n",
        "\tsigmoid_fn = torch.nn.Sigmoid()\n",
        "\tlogits_after_sigmoid = sigmoid_fn(pred_logits)\n",
        "\tbatch_size = logits_after_sigmoid.size()[0]\t\t\n",
        "\tpred_labels = [1 if logits_after_sigmoid[i].item() >= 0.5 else 0 for i in range(batch_size)]\n",
        "\tpred_labels = torch.tensor(pred_labels)\n",
        "\tif LOSS_FN == 'nw':\n",
        "\t\tloss_func1 = torch.nn.BCEWithLogitsLoss()\n",
        "\telse:\n",
        "\t\tloss_func1 = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
        "\t# loss_function = torch.nn.BCEWithLogitsLoss(weight=weights)\n",
        "\tloss_ver = loss_func1(h_root, g_labels_tensor)\n",
        "\tg_labels = [t[1] for t in root]\n",
        "\n",
        "\n",
        "    #CASE 2: Summarization\n",
        "\tloss_func2 = torch.nn.BCELoss(weight=summ_weight)\n",
        "\tsumm_labels = tree_batch['s_gt'].to(device)\n",
        "\tpred_label_vals = summ_out.detach().cpu()\n",
        "\tpred_v,pred_summ_label = torch.max(pred_label_vals,1)\n",
        "\tg_summ_root = summ_labels.to('cpu').tolist()\n",
        "\tg_summ_label = [t[1] for t in g_summ_root]\n",
        "\tloss_summ = loss_func2(summ_out,summ_labels)\n",
        "\n",
        "\n",
        "    #CASE 3: CONTENT-CLASSIFICATION\n",
        "\tloss_func3 = torch.nn.CrossEntropyLoss(weight = cont_weight)\n",
        "\tcont_labels = tree_batch['c_gt'].to(device)\n",
        "\tcont_label_vals = cont_out.detach().cpu()\n",
        "\tcont_v,cont_label = torch.max(cont_label_vals, 1)\n",
        "\tcont_gt = tree_batch['c_gt'] \n",
        "\tloss_cont = loss_func3(cont_out,cont_labels)\n",
        "\n",
        "\n",
        "\tloss = (loss_ver + loss_summ + loss_cont)/3\n",
        "\toptimizer.zero_grad()\n",
        "\tif mode == \"train\":\n",
        "\t\tloss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\t\toptimizer.step()\n",
        "\t\n",
        "\treturn loss, pred_labels, g_labels, pred_summ_label, g_summ_label, cont_label, cont_gt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg-unMu1j-EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing(test_trees,model,epoch):\n",
        "    print('Now Testing:', test_file)\n",
        "    acc = 0\n",
        "    total = 0\n",
        "    predicted = []\n",
        "    ground = []\n",
        "    summ_pred = []\n",
        "    summ_ground = []\n",
        "    cont_pred = []\n",
        "    cont_ground = []\n",
        "    model.eval()\n",
        "    prob = []\n",
        "    pred =[]\n",
        "    cont = []\n",
        "    tweetid = []\n",
        "    with torch.no_grad():\n",
        "        for test in test_trees:\n",
        "            h_test, h_test_root, c, summ_out, cont_out = model(\n",
        "                    test['f'].to(device),\n",
        "                    test['a'].to(device),\n",
        "                    test['k'].to(device),\n",
        "                    test['node_order'].to(device),\n",
        "                    test['adjacency_list'].to(device),\n",
        "                    test['edge_order'].to(device),\n",
        "                    test['root_n'].to(device)\n",
        "            )\n",
        "            \n",
        "            #VERIFICATION\n",
        "            true_label_val = test['root_l'].to('cpu')\t\t\t\t\t\n",
        "            true_label = true_label_val[0][1].item()\n",
        "            pred_logit = h_test_root.detach().cpu()\t\t\t\t\t\n",
        "            logit_after_sigmoid = sigmoid_fn(pred_logit)\n",
        "            pred_label = 1 if logit_after_sigmoid[0].item() >= 0.5 else 0\t\t \n",
        "            predicted.append(pred_label)\n",
        "            ground.append(true_label)\n",
        "            if pred_label == true_label:\n",
        "                acc += 1\n",
        "\n",
        "            #SUMMARIZATION\n",
        "            summ_true_vals = test['s_gt']\n",
        "            summ_pred_vals = summ_out.cpu()\n",
        "            summ_v,summ_label = torch.max(summ_pred_vals, 1)\n",
        "            prob.append(summ_v)\n",
        "            pred.append(summ_label)\n",
        "            tweetid.append(test['tweet_id'])\n",
        "            summ_true_vals = summ_true_vals[0][1]\n",
        "            summ_pred.append(summ_label)\n",
        "            summ_ground.append(summ_true_vals)\n",
        "\n",
        "            #CONTENT-CLASSIFICATION\n",
        "            cont_true_val = test['c_gt']\n",
        "            cont_pred_vals = cont_out.detach().cpu()\n",
        "            cont_v,cont_label = torch.max(cont_pred_vals, 1)\n",
        "            cont_pred.append(cont_label)\n",
        "            cont_ground.append(cont_true_val)\n",
        "\n",
        "            total += 1\n",
        "    \n",
        "    print(\"===================   TESTING   =====================\")\n",
        "    print(test_file, 'accuracy:', acc / total)\n",
        "    print(\"*****VERIFICATION*****\")\n",
        "    print(classification_report(ground, predicted, digits=5))\n",
        "    print('confusion matrix ')\n",
        "    print(confusion_matrix(ground, predicted))    \n",
        "\n",
        "    print(\"*****SUMMARIZATION*****\")\n",
        "    print(classification_report(summ_ground,summ_pred, digits=5))\n",
        "    print('confusion matrix ')\n",
        "    print(confusion_matrix(summ_ground,summ_pred))\n",
        "\n",
        "    print(\"*****CONTENT-CLASSIFICATION*****\")\n",
        "    print(classification_report(cont_ground,cont_pred,digits=5))\n",
        "    print('confusion matrix ')\n",
        "    print(confusion_matrix(cont_ground,cont_pred))\n",
        "\n",
        "\n",
        "    # dfsum = pd.DataFrame({\"tweetid\":tweetid,\"summ_pred\":pred,\"summ_prob\":prob, \"content_label\":cont_pred, \"verification\":predicted})\n",
        "    # dfsum.to_pickle(path+model_name+str(learning_rate)[0]+'/'+test_file[:-4]+\"_\"+str(epoch)+\".pkl\")        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOPprAbGj32Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba16dd38880e4542841a35b71748445a",
            "ebc0924598a14e6e8dbf5e7aa651d7a7",
            "3b11531945434ea3b7d37ea1c759c6b2",
            "fa22966a5de94992931748ac1207a8ae",
            "53823db981354de3ac02c52c0e4f126a",
            "65eff835a61e4e85b8e709de5834d7ba",
            "2b182c9154824af0921f5b33d303f1ee",
            "70a23656d9b143f6a0774662e6d2280f",
            "08aa0a064c18467aaa3216b45da6b3e9",
            "dfc8ad5d80ec472fb88a3be7eb1c1d0f",
            "93f18ff4b5144e05948791cf78626eb0",
            "cc2236ffcb0b48c1b39fad0ff57e5429",
            "f3da8fdbd6bc468f896d3d92a7eefa14",
            "532473790e8743759f40a5018cfd9f78",
            "74b4d14df6584afd9fe7cf64de724e7f",
            "4cadedfd209b4afeb0339f91905b81d7"
          ]
        },
        "outputId": "10926458-5c4b-4582-9099-c9b2d44e84cc"
      },
      "source": [
        "TRAINABLE_LAYERS = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "lr_list = [1e-5, 2e-5]\n",
        "# lr_list = [5e-5]\n",
        "for lr in lr_list:\n",
        "\tprint(\"\\n\\n\\nTraining with LR: \", lr)\n",
        "\t# train_accuracy = []\n",
        "\t# val_accuracy = []\n",
        "\tfor test in files:\n",
        "\t\tseed_val = 40\n",
        "\t\trandom.seed(seed_val)\n",
        "\t\tnumpy.random.seed(seed_val)\n",
        "\t\ttorch.manual_seed(seed_val)\n",
        "\t\ttorch.backends.cudnn.deterministic = True\n",
        "\t\ttorch.backends.cudnn.benchmark = False\n",
        "\t\ttorch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "\t\t# path = \"./Models/\"\n",
        "\t\t# path = \"./drive/My Drive/IIT_Kgp/Research/Disaster/BTP_Chandana_Vishnu/verification/Models/\"\n",
        "\t\tIN_FEATURES = 808\n",
        "\t\tOUT_FEATURES = 128\n",
        "\t\tNUM_ITERATIONS = 5\n",
        "\t\tBATCH_SIZE = 16\n",
        "\t\tCLASSIFIER_DROPOUT = 0.3\n",
        "\t\t# if MODEL_NAME == \"BERT\":\n",
        "\t\t# \tname = path + \"stl_verification_featBERT.pt\"\n",
        "\t\t# elif MODEL_NAME == \"ROBERTA\":\n",
        "\t\t# \tname = path + \"stl_verification_featROBERTA.pt\"\n",
        "\t\t# elif MODEL_NAME == \"XLNET\":\n",
        "\t\t# \tname = path + \"stl_verification_featXLNET.pt\"\n",
        "\t\t\n",
        "\t\tmodel = TreeLSTM(MODEL_NAME, TRAINABLE_LAYERS, IN_FEATURES, OUT_FEATURES, CLASSIFIER_DROPOUT, mode=\"cls\")\n",
        "\t\tmodel.cuda()\n",
        "\t\t# model.cuda()\n",
        "\t\t# test_model = TreeLSTM(MODEL_NAME, TRAINABLE_LAYERS, IN_FEATURES, OUT_FEATURES, CLASSIFIER_DROPOUT, mode=\"cls\")\n",
        "\t\t# test_model.cuda(gpu_id)\n",
        "\t\t# test_model.cuda()\n",
        "\t\t\n",
        "\t\tif OPTIM == 'adam':\n",
        "\t\t\tif L2_REGULARIZER == 'n':\n",
        "\t\t\t\toptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\t\t\telse:\n",
        "\t\t\t\toptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\t\telse:\n",
        "\t\t\toptimizer = torch.optim.AdamW(model.parameters(), lr=lr, amsgrad=True)\n",
        "\n",
        "\t\tsigmoid_fn = torch.nn.Sigmoid()\n",
        "\t\t\n",
        "\t\t# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, verbose=True)\t\t\n",
        "\n",
        "\t\ttest_file = test\n",
        "\t\tprint('Training Set:', set(files) - {test_file})\n",
        "\t\ttest_trees = []\n",
        "\t\ttrain_trees = []\n",
        "\t\tval_data = []\n",
        "\t\tfor filename in files:\n",
        "\t\t\tif filename == test:\n",
        "\t\t\t\ttest_trees.extend(tree_li[filename])\n",
        "\t\t\t\ttest_trees.extend(val_li[filename])\n",
        "\t\t\telse:\n",
        "\t\t\t\tcurr_tree_dataset = TreeDataset(tree_li[filename])\n",
        "\t\t\t\ttrain_trees.extend(curr_tree_dataset)\n",
        "\t\t\t\tval_data.extend(TreeDataset(val_li[filename]))\n",
        "\t\t\n",
        "\t\tprint(\"Size of test data\", len(test_trees))\n",
        "\t\t# print(\"size of training data\", sum([len(i) for i in (train_trees)]))\n",
        "\t\tprint(\"size of training data\", len(train_trees))\n",
        "\t\tprint(\"\\ntraining started....\")\n",
        "\t\t\n",
        "\t\tprev_loss = 1\n",
        "\t\tprev_acc = 0\t\t\n",
        "\t\tfor i in range(NUM_ITERATIONS):\n",
        "\t\t\t\n",
        "\t\t\tmodel.train()\t\t\t\n",
        "\t\t\t\n",
        "\t\t\tdata_gen = DataLoader(\n",
        "\t\t\t\ttrain_trees,\n",
        "\t\t\t\tcollate_fn=batch_tree_input,\n",
        "\t\t\t\tbatch_size=BATCH_SIZE,\n",
        "\t\t\t\tshuffle = True\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tval_gen = DataLoader(\n",
        "\t\t\t\tval_data,\n",
        "\t\t\t\tcollate_fn=batch_tree_input,\n",
        "\t\t\t\tbatch_size=BATCH_SIZE,\n",
        "\t\t\t\tshuffle = True\n",
        "\t\t\t)\n",
        "\t\t\t\n",
        "\t\t\tver_gl = []\n",
        "\t\t\tver_pl = []\n",
        "\t\t\tcont_gl = []\n",
        "\t\t\tcont_pl = []\n",
        "\t\t\tsumm_gl = []\n",
        "\t\t\tsumm_pl = []\n",
        "\n",
        "\n",
        "\t\t\tval_ver_gl = []\n",
        "\t\t\tval_ver_pl = []\n",
        "\t\t\tval_cont_gl = []\n",
        "\t\t\tval_cont_pl = []\n",
        "\t\t\tval_summ_gl = []\n",
        "\t\t\tval_summ_pl = []\n",
        "\n",
        "\t\t\tj = 0\n",
        "\t\t\ttrain_avg_loss=0\t\t\t\t\t\n",
        "\t\t\terr_count = 0\n",
        "\t\t\tfor tree_batch in data_gen:\n",
        "\t\t\t\tloss, v_pl, v_gl, s_pl, s_gl, c_pl, c_gl = train(tree_batch, test_file, \"train\")\n",
        "\t\t\t\terr = 0\n",
        "\t\t\t\terr_count += err\n",
        "\t\t\t\tif err != 1:\n",
        "\t\t\t\t\tver_gl.extend(v_gl)\n",
        "\t\t\t\t\tver_pl.extend(v_pl)\n",
        "\t\t\t\t\tcont_gl.extend(c_gl)\n",
        "\t\t\t\t\tcont_pl.extend(c_pl)\n",
        "\t\t\t\t\tsumm_gl.extend(s_gl)\n",
        "\t\t\t\t\tsumm_pl.extend(s_pl)\n",
        "\t\t\t\t\tj += 1\n",
        "\t\t\t\t\ttrain_avg_loss += loss.item()\t\t\t\t\t\n",
        "\t\t\t\t# torch.cuda.empty_cache()\n",
        "\t\t\tacc1 = accuracy_score(cont_gl,cont_pl)\n",
        "\t\t\tacc2 = accuracy_score(ver_gl,ver_pl)\n",
        "\t\t\tacc3 = accuracy_score(summ_gl,summ_pl)\n",
        "\t\t\ttrain_acc = (acc1 + acc2 + acc3)/3\n",
        "\t\t\t# train_acc = accuracy_score(ground_labels, predicted_labels)\n",
        "\t\t\ttrain_avg_loss /= j\n",
        "\t\t\t\n",
        "\t\t\tprint(\"validation started..\",len(val_data))\n",
        "\t\t\tmodel.eval()\n",
        "\t\t\tval_j = 0\n",
        "\t\t\tval_avg_loss = 0\t\t\t\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tfor batch in val_gen:\n",
        "\t\t\t\t\tloss, v_pl, v_gl, s_pl, s_gl, c_pl, c_gl = train(batch, test_file, \"eval\")\n",
        "\t\t\t\t\terr = 0\n",
        "\t\t\t\t\terr_count += err\n",
        "\t\t\t\t\tif err != 1:\n",
        "\t\t\t\t\t\tval_ver_gl.extend(v_gl)\n",
        "\t\t\t\t\t\tval_ver_pl.extend(v_pl)\n",
        "\t\t\t\t\t\tval_cont_gl.extend(c_gl)\n",
        "\t\t\t\t\t\tval_cont_pl.extend(c_pl)\n",
        "\t\t\t\t\t\tval_summ_gl.extend(s_gl)\n",
        "\t\t\t\t\t\tval_summ_pl.extend(s_pl)\n",
        "\t\t\t\t\t\tval_j += 1\n",
        "\t\t\t\t\t\tval_avg_loss += loss.item()\n",
        "\t\t\t\t\t# torch.cuda.empty_cache()\t\t\t\n",
        "\t\t\t# val_acc = accuracy_score(val_ground_labels, val_predicted_labels)\n",
        "\t\t\tval_acc1 = accuracy_score(val_cont_gl,val_cont_pl)\n",
        "\t\t\tval_acc2 = accuracy_score(val_ver_gl,val_ver_pl)\n",
        "\t\t\tval_acc3 = accuracy_score(val_summ_gl,val_summ_pl)\n",
        "\t\t\tval_acc = (val_acc1 + val_acc2 + val_acc3)/3\n",
        "\t\t\t# val_f1 = f1_score(val_ground_labels, val_predicted_labels)\n",
        "\t\t\tval_avg_loss /= val_j\n",
        "\t\t\t\n",
        "\t\t\tif MODEL_SAVING_POLICY == \"acc\":\n",
        "\t\t\t\tif(prev_acc <= val_acc):\n",
        "\t\t\t\t\tsave_model(model, name, val_acc, val_avg_loss)\n",
        "\t\t\t\t\tprev_acc = val_acc\n",
        "\t\t\telse:\t\t\t\n",
        "\t\t\t\tif(prev_loss >= val_avg_loss):\n",
        "\t\t\t\t\tsave_model(model, name, val_acc, val_avg_loss)\n",
        "\t\t\t\t\tprev_loss = val_avg_loss\n",
        "\t\t\t\n",
        "\t\t\tprint('Iteration ', i)\n",
        "\t\t\tprint(\"errors \",err_count)\t\t\t\n",
        "\t\t\tprint('Training Loss: ', train_avg_loss)\n",
        "\t\t\tprint('Training accuracy: ', train_acc)\t\n",
        "\t\t\tprint('Validation loss: ', val_avg_loss)\t\t\t\n",
        "\t\t\tprint('Validation accuracy: ', val_acc)\n",
        "\t\t\tprint('Verification accuracy ',acc1)\n",
        "\t\t\tprint('Summary accuracy ',acc2)\n",
        "\t\t\tprint('content classification accuracy ',acc3)\n",
        "\t\t\t# print('Validation f1 score: ', val_f1)\n",
        "\t\t\tprint('Training confusion matrix: ')\n",
        "\t\t\tprint(\"*********VERIFICATION********\")\n",
        "\t\t\tprint(confusion_matrix(ver_gl, ver_pl))\n",
        "\t\t\tprint(\"*********SUMMARIZATION********\")\n",
        "\t\t\tprint(confusion_matrix(summ_gl, summ_pl))\n",
        "\t\t\tprint(\"*********CONTENT-CLASSIFICATION********\")\n",
        "\t\t\tprint(confusion_matrix(cont_gl, cont_pl))\n",
        "            \n",
        "            # print(classification_report())\n",
        "\t\t\t# train_accuracy.append(train_acc)\n",
        "\t\t\t# val_accuracy.append(val_acc)\n",
        "\t\t\t# scheduler.step(val_acc)\n",
        "\n",
        "\t\t\tif ((i+1) % 5 == 0 and i > 0):\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\tsave_model(model,name2,val_acc)\n",
        "\t\t\t\t\toutput = load_model(model,name)\n",
        "\t\t\t\t\ttesting(test_trees,model,i+1)\n",
        "\t\t\t\toutput = load_model(model,name2)\n",
        "\t\t\t\t\n",
        "\t\t# plt.plot(numpy.array(train_accuracy))\n",
        "\t\t# plt.plot(numpy.array(val_accuracy))\n",
        "\t\t# plt.legend(['train_acc','val_acc'])\n",
        "\t\t# plt.show()\n",
        "\t\t# print('Iteration ', i+1,' Loss: ', total_loss)\n",
        "\t\tprint('Training and Testing Completed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Training with LR:  1e-05\n",
            "model intialising...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba16dd38880e4542841a35b71748445a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08aa0a064c18467aaa3216b45da6b3e9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Set: {'ottawashooting.txt', 'sydneysiege.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 2079\n",
            "size of training data 2066\n",
            "\n",
            "training started....\n",
            "validation started.. 514\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.8732669637753413\n",
            "Training accuracy:  0.6658599548241368\n",
            "Validation loss:  0.7199753459655878\n",
            "Validation accuracy:  0.7645914396887159\n",
            "Verification accuracy  0.521297192642788\n",
            "Summary accuracy  0.6360116166505324\n",
            "content classification accuracy  0.8402710551790901\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[596 481]\n",
            " [271 718]]\n",
            "*********SUMMARIZATION********\n",
            "[[1728   45]\n",
            " [ 285    8]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[172  94  40  13]\n",
            " [100 243 108  30]\n",
            " [ 77 126 150  57]\n",
            " [ 47 198  99 512]]\n",
            "validation started.. 514\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.638381478878168\n",
            "Training accuracy:  0.7917070022587932\n",
            "Validation loss:  0.618401129137386\n",
            "Validation accuracy:  0.8073929961089495\n",
            "Verification accuracy  0.7410454985479187\n",
            "Summary accuracy  0.7584704743465635\n",
            "content classification accuracy  0.8756050338818974\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[834 243]\n",
            " [256 733]]\n",
            "*********SUMMARIZATION********\n",
            "[[1744   29]\n",
            " [ 228   65]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[267  20  22  10]\n",
            " [ 68 303  94  16]\n",
            " [ 55  52 264  39]\n",
            " [ 28  49  82 697]]\n",
            "validation started.. 514\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.5011546611785889\n",
            "Training accuracy:  0.8425298483381735\n",
            "Validation loss:  0.5986481927561037\n",
            "Validation accuracy:  0.8184176394293127\n",
            "Verification accuracy  0.7986447241045499\n",
            "Summary accuracy  0.8170377541142304\n",
            "content classification accuracy  0.9119070667957405\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[892 185]\n",
            " [193 796]]\n",
            "*********SUMMARIZATION********\n",
            "[[1739   34]\n",
            " [ 148  145]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[279  22  15   3]\n",
            " [ 42 348  81  10]\n",
            " [ 45  49 286  30]\n",
            " [ 15  38  66 737]]\n",
            "validation started.. 514\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.4029413191171793\n",
            "Training accuracy:  0.8777024846724749\n",
            "Validation loss:  0.6090706966140054\n",
            "Validation accuracy:  0.8268482490272374\n",
            "Verification accuracy  0.8339787028073572\n",
            "Summary accuracy  0.8581800580832527\n",
            "content classification accuracy  0.9409486931268151\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[939 138]\n",
            " [155 834]]\n",
            "*********SUMMARIZATION********\n",
            "[[1737   36]\n",
            " [  86  207]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[284  18  14   3]\n",
            " [ 39 373  61   8]\n",
            " [ 35  30 324  21]\n",
            " [ 11  38  65 742]]\n",
            "validation started.. 514\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.31458245212068925\n",
            "Training accuracy:  0.9075508228460794\n",
            "Validation loss:  0.7178643011685574\n",
            "Validation accuracy:  0.8197146562905319\n",
            "Verification accuracy  0.8736689254598258\n",
            "Summary accuracy  0.8852855759922555\n",
            "content classification accuracy  0.9636979670861568\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[970 107]\n",
            " [130 859]]\n",
            "*********SUMMARIZATION********\n",
            "[[1750   23]\n",
            " [  52  241]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[299   9  10   1]\n",
            " [ 23 419  34   5]\n",
            " [ 30  31 328  21]\n",
            " [ 10  29  58 759]]\n",
            "Validation accuracy of the model is  0.8268482490272374\n",
            "Validation loss of the model is  0.6090706966140054\n",
            "Now Testing: charliehebdo.txt\n",
            "===================   TESTING   =====================\n",
            "charliehebdo.txt accuracy: 0.8128908128908129\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.95630   0.79642   0.86907      1621\n",
            "           1    0.54733   0.87118   0.67228       458\n",
            "\n",
            "    accuracy                        0.81289      2079\n",
            "   macro avg    0.75181   0.83380   0.77068      2079\n",
            "weighted avg    0.86620   0.81289   0.82572      2079\n",
            "\n",
            "confusion matrix \n",
            "[[1291  330]\n",
            " [  59  399]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.80150   0.97684   0.88053      1641\n",
            "         1.0    0.51899   0.09361   0.15861       438\n",
            "\n",
            "    accuracy                        0.79076      2079\n",
            "   macro avg    0.66024   0.53523   0.51957      2079\n",
            "weighted avg    0.74198   0.79076   0.72843      2079\n",
            "\n",
            "confusion matrix \n",
            "[[1603   38]\n",
            " [ 397   41]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.55128   0.62019   0.58371       208\n",
            "           1    0.63659   0.67196   0.65380       378\n",
            "           2    0.36321   0.45562   0.40420       169\n",
            "           3    0.94246   0.87840   0.90930      1324\n",
            "\n",
            "    accuracy                        0.78066      2079\n",
            "   macro avg    0.62339   0.65654   0.63775      2079\n",
            "weighted avg    0.80063   0.78066   0.78921      2079\n",
            "\n",
            "confusion matrix \n",
            "[[ 129   37   21   21]\n",
            " [  88  254   30    6]\n",
            " [   3   45   77   44]\n",
            " [  14   63   84 1163]]\n",
            "Validation accuracy of the model is  0.8197146562905319\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'ottawashooting.txt', 'sydneysiege.txt', 'charliehebdo.txt'}\n",
            "Size of test data 469\n",
            "size of training data 3354\n",
            "\n",
            "training started....\n",
            "validation started.. 836\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7617550620010921\n",
            "Training accuracy:  0.7286821705426356\n",
            "Validation loss:  0.601964829103002\n",
            "Validation accuracy:  0.8185805422647529\n",
            "Verification accuracy  0.6586165772212283\n",
            "Summary accuracy  0.6985688729874776\n",
            "content classification accuracy  0.8288610614192009\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1998  199]\n",
            " [ 812  345]]\n",
            "*********SUMMARIZATION********\n",
            "[[2733   67]\n",
            " [ 507   47]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 235  130   33   29]\n",
            " [  92  428  106   33]\n",
            " [  40  133  214   78]\n",
            " [  63  236  172 1332]]\n",
            "validation started.. 836\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.5442081385425159\n",
            "Training accuracy:  0.825084476247267\n",
            "Validation loss:  0.5667867373745397\n",
            "Validation accuracy:  0.8189792663476873\n",
            "Verification accuracy  0.8112701252236136\n",
            "Summary accuracy  0.8050089445438283\n",
            "content classification accuracy  0.8589743589743589\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1935  262]\n",
            " [ 392  765]]\n",
            "*********SUMMARIZATION********\n",
            "[[2708   92]\n",
            " [ 381  173]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 351   44   21   11]\n",
            " [  73  469  103   14]\n",
            " [  39   45  332   49]\n",
            " [  32   70  132 1569]]\n",
            "validation started.. 836\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.4392787612619854\n",
            "Training accuracy:  0.8606638839196977\n",
            "Validation loss:  0.5735283876365086\n",
            "Validation accuracy:  0.8301435406698565\n",
            "Verification accuracy  0.847644603458557\n",
            "Summary accuracy  0.8416815742397138\n",
            "content classification accuracy  0.8926654740608229\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1946  251]\n",
            " [ 280  877]]\n",
            "*********SUMMARIZATION********\n",
            "[[2705   95]\n",
            " [ 265  289]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 367   39   16    5]\n",
            " [  53  528   73    5]\n",
            " [  24   37  375   29]\n",
            " [  35   65  130 1573]]\n",
            "validation started.. 836\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.35082877654404865\n",
            "Training accuracy:  0.8935599284436493\n",
            "Validation loss:  0.5550712782819316\n",
            "Validation accuracy:  0.8393141945773525\n",
            "Verification accuracy  0.8786523553965414\n",
            "Summary accuracy  0.8729874776386404\n",
            "content classification accuracy  0.9290399522957663\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1986  211]\n",
            " [ 215  942]]\n",
            "*********SUMMARIZATION********\n",
            "[[2727   73]\n",
            " [ 165  389]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 389   20   15    3]\n",
            " [  44  555   54    6]\n",
            " [  16   28  392   29]\n",
            " [  27   55  110 1611]]\n",
            "validation started.. 836\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.26503013524980773\n",
            "Training accuracy:  0.9251639833035181\n",
            "Validation loss:  0.638667661626384\n",
            "Validation accuracy:  0.8393141945773525\n",
            "Verification accuracy  0.9090638044126417\n",
            "Summary accuracy  0.9117471675611211\n",
            "content classification accuracy  0.9546809779367919\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[2035  162]\n",
            " [ 134 1023]]\n",
            "*********SUMMARIZATION********\n",
            "[[2752   48]\n",
            " [ 104  450]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 393   20   11    3]\n",
            " [  24  597   34    4]\n",
            " [  16   14  422   13]\n",
            " [  22   46   98 1637]]\n",
            "Validation accuracy of the model is  0.8393141945773525\n",
            "Validation loss of the model is  0.638667661626384\n",
            "Now Testing: germanwings-crash.txt\n",
            "===================   TESTING   =====================\n",
            "germanwings-crash.txt accuracy: 0.673773987206823\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.84211   0.41558   0.55652       231\n",
            "           1    0.61972   0.92437   0.74199       238\n",
            "\n",
            "    accuracy                        0.67377       469\n",
            "   macro avg    0.73091   0.66998   0.64926       469\n",
            "weighted avg    0.72925   0.67377   0.65064       469\n",
            "\n",
            "confusion matrix \n",
            "[[ 96 135]\n",
            " [ 18 220]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.76674   0.99440   0.86585       357\n",
            "         1.0    0.66667   0.03571   0.06780       112\n",
            "\n",
            "    accuracy                        0.76546       469\n",
            "   macro avg    0.71670   0.51506   0.46683       469\n",
            "weighted avg    0.74284   0.76546   0.67527       469\n",
            "\n",
            "confusion matrix \n",
            "[[355   2]\n",
            " [108   4]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.39752   0.85333   0.54237        75\n",
            "           1    0.58333   0.09524   0.16374       147\n",
            "           2    0.31844   0.53774   0.40000       106\n",
            "           3    0.85714   0.63830   0.73171       141\n",
            "\n",
            "    accuracy                        0.47974       469\n",
            "   macro avg    0.53911   0.53115   0.45946       469\n",
            "weighted avg    0.57607   0.47974   0.44844       469\n",
            "\n",
            "confusion matrix \n",
            "[[64  2  9  0]\n",
            " [44 14 81  8]\n",
            " [38  4 57  7]\n",
            " [15  4 32 90]]\n",
            "Validation accuracy of the model is  0.8393141945773525\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'sydneysiege.txt', 'charliehebdo.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 890\n",
            "size of training data 3018\n",
            "\n",
            "training started....\n",
            "validation started.. 751\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7870782629522697\n",
            "Training accuracy:  0.7083057212281864\n",
            "Validation loss:  0.6240330027773026\n",
            "Validation accuracy:  0.7878384376387039\n",
            "Verification accuracy  0.6153081510934394\n",
            "Summary accuracy  0.7047713717693836\n",
            "content classification accuracy  0.8048376408217363\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1917  127]\n",
            " [ 764  210]]\n",
            "*********SUMMARIZATION********\n",
            "[[2396   70]\n",
            " [ 519   33]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 164   95   59   27]\n",
            " [ 110  302  171   40]\n",
            " [  68   89  195   62]\n",
            " [  61  113  266 1196]]\n",
            "validation started.. 751\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.5645428577428142\n",
            "Training accuracy:  0.8114645460569915\n",
            "Validation loss:  0.5583951026201248\n",
            "Validation accuracy:  0.8162450066577897\n",
            "Verification accuracy  0.7952286282306164\n",
            "Summary accuracy  0.7866136514247847\n",
            "content classification accuracy  0.8525513585155732\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1800  244]\n",
            " [ 400  574]]\n",
            "*********SUMMARIZATION********\n",
            "[[2401   65]\n",
            " [ 380  172]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 263   37   36    9]\n",
            " [  67  440  105   11]\n",
            " [  35   55  281   43]\n",
            " [  30   65  125 1416]]\n",
            "validation started.. 751\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.44700049739035347\n",
            "Training accuracy:  0.854760326927325\n",
            "Validation loss:  0.5564724644447895\n",
            "Validation accuracy:  0.8162450066577897\n",
            "Verification accuracy  0.8439363817097415\n",
            "Summary accuracy  0.8359840954274353\n",
            "content classification accuracy  0.8843605036447979\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1811  233]\n",
            " [ 262  712]]\n",
            "*********SUMMARIZATION********\n",
            "[[2368   98]\n",
            " [ 251  301]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 292   29   20    4]\n",
            " [  36  515   67    5]\n",
            " [  28   36  310   40]\n",
            " [  27   64  115 1430]]\n",
            "validation started.. 751\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.35376371435387427\n",
            "Training accuracy:  0.8935277225535675\n",
            "Validation loss:  0.5658344096959905\n",
            "Validation accuracy:  0.8295605858854861\n",
            "Verification accuracy  0.8840291583830351\n",
            "Summary accuracy  0.8730947647448641\n",
            "content classification accuracy  0.9234592445328031\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1852  192]\n",
            " [ 191  783]]\n",
            "*********SUMMARIZATION********\n",
            "[[2391   75]\n",
            " [ 156  396]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 304   21   15    5]\n",
            " [  24  532   63    4]\n",
            " [  15   25  355   19]\n",
            " [  15   52   92 1477]]\n",
            "validation started.. 751\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.2731740676536762\n",
            "Training accuracy:  0.9204771371769384\n",
            "Validation loss:  0.623180702962774\n",
            "Validation accuracy:  0.833555259653795\n",
            "Verification accuracy  0.9102054340622929\n",
            "Summary accuracy  0.9015904572564613\n",
            "content classification accuracy  0.949635520212061\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1893  151]\n",
            " [ 146  828]]\n",
            "*********SUMMARIZATION********\n",
            "[[2419   47]\n",
            " [ 105  447]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 317   15   11    2]\n",
            " [  22  560   34    7]\n",
            " [  15   20  366   13]\n",
            " [  18   32   82 1504]]\n",
            "Validation accuracy of the model is  0.833555259653795\n",
            "Validation loss of the model is  0.623180702962774\n",
            "Now Testing: ottawashooting.txt\n",
            "===================   TESTING   =====================\n",
            "ottawashooting.txt accuracy: 0.7134831460674157\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.64249   0.88571   0.74474       420\n",
            "           1    0.84566   0.55957   0.67350       470\n",
            "\n",
            "    accuracy                        0.71348       890\n",
            "   macro avg    0.74407   0.72264   0.70912       890\n",
            "weighted avg    0.74978   0.71348   0.70712       890\n",
            "\n",
            "confusion matrix \n",
            "[[372  48]\n",
            " [207 263]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.89888   0.82581   0.86079       775\n",
            "         1.0    0.24157   0.37391   0.29352       115\n",
            "\n",
            "    accuracy                        0.76742       890\n",
            "   macro avg    0.57022   0.59986   0.57715       890\n",
            "weighted avg    0.81394   0.76742   0.78749       890\n",
            "\n",
            "confusion matrix \n",
            "[[640 135]\n",
            " [ 72  43]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.76744   0.54696   0.63871       181\n",
            "           1    0.54167   0.53061   0.53608       196\n",
            "           2    0.53271   0.67456   0.59530       169\n",
            "           3    0.83099   0.85756   0.84406       344\n",
            "\n",
            "    accuracy                        0.68764       890\n",
            "   macro avg    0.66820   0.65242   0.65354       890\n",
            "weighted avg    0.69771   0.68764   0.68724       890\n",
            "\n",
            "confusion matrix \n",
            "[[ 99  52  14  16]\n",
            " [ 19 104  52  21]\n",
            " [  7  25 114  23]\n",
            " [  4  11  34 295]]\n",
            "Validation accuracy of the model is  0.833555259653795\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'ottawashooting.txt', 'charliehebdo.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 1221\n",
            "size of training data 2752\n",
            "\n",
            "training started....\n",
            "validation started.. 686\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7846339736566987\n",
            "Training accuracy:  0.7003391472868218\n",
            "Validation loss:  0.6172733223715494\n",
            "Validation accuracy:  0.7886297376093294\n",
            "Verification accuracy  0.6090116279069767\n",
            "Summary accuracy  0.6951308139534884\n",
            "content classification accuracy  0.796875\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1663  156]\n",
            " [ 683  250]]\n",
            "*********SUMMARIZATION********\n",
            "[[2159   60]\n",
            " [ 499   34]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 217  104   33   16]\n",
            " [ 159  269  120   29]\n",
            " [  53  100  156   64]\n",
            " [  72  181  145 1034]]\n",
            "validation started.. 686\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.548301357169484\n",
            "Training accuracy:  0.8138323643410853\n",
            "Validation loss:  0.5481753314650336\n",
            "Validation accuracy:  0.8236151603498542\n",
            "Verification accuracy  0.7859738372093024\n",
            "Summary accuracy  0.8121366279069767\n",
            "content classification accuracy  0.8433866279069767\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1631  188]\n",
            " [ 329  604]]\n",
            "*********SUMMARIZATION********\n",
            "[[2141   78]\n",
            " [ 353  180]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 307   42   16    5]\n",
            " [  78  377  116    6]\n",
            " [  28   50  256   39]\n",
            " [  29   61  119 1223]]\n",
            "validation started.. 686\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.43678409493593284\n",
            "Training accuracy:  0.8634932170542635\n",
            "Validation loss:  0.5355714705101279\n",
            "Validation accuracy:  0.8304178814382897\n",
            "Verification accuracy  0.8477470930232558\n",
            "Summary accuracy  0.860828488372093\n",
            "content classification accuracy  0.8819040697674418\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1660  159]\n",
            " [ 224  709]]\n",
            "*********SUMMARIZATION********\n",
            "[[2125   94]\n",
            " [ 231  302]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 322   30   14    4]\n",
            " [  43  457   73    4]\n",
            " [  20   36  293   24]\n",
            " [  24   54   93 1261]]\n",
            "validation started.. 686\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.3438737293662027\n",
            "Training accuracy:  0.8961967054263565\n",
            "Validation loss:  0.5312681901593541\n",
            "Validation accuracy:  0.8381924198250729\n",
            "Verification accuracy  0.8789970930232558\n",
            "Summary accuracy  0.8924418604651163\n",
            "content classification accuracy  0.9171511627906976\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1677  142]\n",
            " [ 154  779]]\n",
            "*********SUMMARIZATION********\n",
            "[[2138   81]\n",
            " [ 147  386]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 338   19   11    2]\n",
            " [  29  476   67    5]\n",
            " [  14   24  317   18]\n",
            " [  18   42   84 1288]]\n",
            "validation started.. 686\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.2594000672228461\n",
            "Training accuracy:  0.9234496124031008\n",
            "Validation loss:  0.5594390907952952\n",
            "Validation accuracy:  0.8401360544217686\n",
            "Verification accuracy  0.9022529069767442\n",
            "Summary accuracy  0.9182412790697675\n",
            "content classification accuracy  0.9498546511627907\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1706  113]\n",
            " [ 112  821]]\n",
            "*********SUMMARIZATION********\n",
            "[[2173   46]\n",
            " [  92  441]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 342   14   12    2]\n",
            " [  25  510   40    2]\n",
            " [  14   14  331   14]\n",
            " [  24   39   69 1300]]\n",
            "Validation accuracy of the model is  0.8401360544217686\n",
            "Validation loss of the model is  0.5594390907952952\n",
            "Now Testing: sydneysiege.txt\n",
            "===================   TESTING   =====================\n",
            "sydneysiege.txt accuracy: 0.7428337428337428\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.73795   0.85408   0.79178       699\n",
            "           1    0.75243   0.59387   0.66381       522\n",
            "\n",
            "    accuracy                        0.74283      1221\n",
            "   macro avg    0.74519   0.72397   0.72779      1221\n",
            "weighted avg    0.74414   0.74283   0.73707      1221\n",
            "\n",
            "confusion matrix \n",
            "[[597 102]\n",
            " [212 310]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.91381   0.86150   0.88688      1083\n",
            "         1.0    0.25000   0.36232   0.29586       138\n",
            "\n",
            "    accuracy                        0.80508      1221\n",
            "   macro avg    0.58190   0.61191   0.59137      1221\n",
            "weighted avg    0.83878   0.80508   0.82008      1221\n",
            "\n",
            "confusion matrix \n",
            "[[933 150]\n",
            " [ 88  50]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.55952   0.31757   0.40517       148\n",
            "           1    0.46301   0.70711   0.55960       239\n",
            "           2    0.47234   0.49554   0.48366       224\n",
            "           3    0.91434   0.80492   0.85615       610\n",
            "\n",
            "    accuracy                        0.66994      1221\n",
            "   macro avg    0.60230   0.58128   0.57615      1221\n",
            "weighted avg    0.70190   0.66994   0.67510      1221\n",
            "\n",
            "confusion matrix \n",
            "[[ 47  88   5   8]\n",
            " [  9 169  48  13]\n",
            " [ 23  65 111  25]\n",
            " [  5  43  71 491]]\n",
            "Validation accuracy of the model is  0.8401360544217686\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "\n",
            "\n",
            "\n",
            "Training with LR:  2e-05\n",
            "model intialising...\n",
            "Training Set: {'ottawashooting.txt', 'sydneysiege.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 2079\n",
            "size of training data 2066\n",
            "\n",
            "training started....\n",
            "validation started.. 514\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7964435990040118\n",
            "Training accuracy:  0.7090997095837367\n",
            "Validation loss:  0.6504577181556008\n",
            "Validation accuracy:  0.7996108949416342\n",
            "Verification accuracy  0.5856727976766699\n",
            "Summary accuracy  0.6931268151016456\n",
            "content classification accuracy  0.8484995159728945\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[687 390]\n",
            " [244 745]]\n",
            "*********SUMMARIZATION********\n",
            "[[1742   31]\n",
            " [ 282   11]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[204  81  22  12]\n",
            " [ 84 251 112  34]\n",
            " [ 64 102 190  54]\n",
            " [ 34 170  87 565]]\n",
            "validation started.. 514\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.5459287505883437\n",
            "Training accuracy:  0.8286544046466603\n",
            "Validation loss:  0.5829274347334197\n",
            "Validation accuracy:  0.8249027237354086\n",
            "Verification accuracy  0.78702807357212\n",
            "Summary accuracy  0.808809293320426\n",
            "content classification accuracy  0.8901258470474347\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[889 188]\n",
            " [207 782]]\n",
            "*********SUMMARIZATION********\n",
            "[[1718   55]\n",
            " [ 172  121]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[278  19  16   6]\n",
            " [ 57 347  65  12]\n",
            " [ 50  52 277  31]\n",
            " [ 19  43  70 724]]\n",
            "validation started.. 514\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.3879375607921527\n",
            "Training accuracy:  0.8888351080993869\n",
            "Validation loss:  0.6159766013875152\n",
            "Validation accuracy:  0.8197146562905319\n",
            "Verification accuracy  0.8533397870280736\n",
            "Summary accuracy  0.8741529525653436\n",
            "content classification accuracy  0.9390125847047435\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[951 126]\n",
            " [134 855]]\n",
            "*********SUMMARIZATION********\n",
            "[[1740   33]\n",
            " [  93  200]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[290  18  10   1]\n",
            " [ 29 387  57   8]\n",
            " [ 36  37 316  21]\n",
            " [ 13  31  42 770]]\n",
            "validation started.. 514\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.26484912450496967\n",
            "Training accuracy:  0.9288480154888674\n",
            "Validation loss:  0.6588070654507839\n",
            "Validation accuracy:  0.8255512321660182\n",
            "Verification accuracy  0.9017424975798645\n",
            "Summary accuracy  0.914811229428848\n",
            "content classification accuracy  0.9699903194578896\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[994  83]\n",
            " [ 93 896]]\n",
            "*********SUMMARIZATION********\n",
            "[[1754   19]\n",
            " [  43  250]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[293  13  10   3]\n",
            " [ 18 432  27   4]\n",
            " [ 28  17 351  14]\n",
            " [  6  23  40 787]]\n",
            "validation started.. 514\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.1779091984893267\n",
            "Training accuracy:  0.9515972894482091\n",
            "Validation loss:  0.8060978493004134\n",
            "Validation accuracy:  0.8365758754863813\n",
            "Verification accuracy  0.9283639883833494\n",
            "Summary accuracy  0.9390125847047435\n",
            "content classification accuracy  0.9874152952565344\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1012   65]\n",
            " [  61  928]]\n",
            "*********SUMMARIZATION********\n",
            "[[1769    4]\n",
            " [  22  271]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[298  12   8   1]\n",
            " [ 11 447  16   7]\n",
            " [ 15  10 373  12]\n",
            " [  4  22  30 800]]\n",
            "Validation accuracy of the model is  0.8365758754863813\n",
            "Validation loss of the model is  0.8060978493004134\n",
            "Now Testing: charliehebdo.txt\n",
            "===================   TESTING   =====================\n",
            "charliehebdo.txt accuracy: 0.81000481000481\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.93108   0.81678   0.87019      1621\n",
            "           1    0.54795   0.78603   0.64574       458\n",
            "\n",
            "    accuracy                        0.81000      2079\n",
            "   macro avg    0.73951   0.80140   0.75797      2079\n",
            "weighted avg    0.84668   0.81000   0.82075      2079\n",
            "\n",
            "confusion matrix \n",
            "[[1324  297]\n",
            " [  98  360]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.80280   0.94272   0.86715      1641\n",
            "         1.0    0.38158   0.13242   0.19661       438\n",
            "\n",
            "    accuracy                        0.77201      2079\n",
            "   macro avg    0.59219   0.53757   0.53188      2079\n",
            "weighted avg    0.71406   0.77201   0.72588      2079\n",
            "\n",
            "confusion matrix \n",
            "[[1547   94]\n",
            " [ 380   58]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.57674   0.59615   0.58629       208\n",
            "           1    0.70064   0.58201   0.63584       378\n",
            "           2    0.35938   0.40828   0.38227       169\n",
            "           3    0.91090   0.93429   0.92245      1324\n",
            "\n",
            "    accuracy                        0.79365      2079\n",
            "   macro avg    0.63691   0.63018   0.63171      2079\n",
            "weighted avg    0.79440   0.79365   0.79279      2079\n",
            "\n",
            "confusion matrix \n",
            "[[ 124   29   25   30]\n",
            " [  83  220   46   29]\n",
            " [   2   36   69   62]\n",
            " [   6   29   52 1237]]\n",
            "Validation accuracy of the model is  0.8365758754863813\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'ottawashooting.txt', 'sydneysiege.txt', 'charliehebdo.txt'}\n",
            "Size of test data 469\n",
            "size of training data 3354\n",
            "\n",
            "training started....\n",
            "validation started.. 836\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.706036937946365\n",
            "Training accuracy:  0.7608825283243887\n",
            "Validation loss:  0.5808593622356091\n",
            "Validation accuracy:  0.8341307814992026\n",
            "Verification accuracy  0.7081097197376267\n",
            "Summary accuracy  0.7382230172927847\n",
            "content classification accuracy  0.8363148479427549\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1946  251]\n",
            " [ 627  530]]\n",
            "*********SUMMARIZATION********\n",
            "[[2731   69]\n",
            " [ 480   74]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 268   98   34   27]\n",
            " [  88  434  100   37]\n",
            " [  38  113  239   75]\n",
            " [  50  142  177 1434]]\n",
            "validation started.. 836\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.4870122964183489\n",
            "Training accuracy:  0.8463526137944744\n",
            "Validation loss:  0.5594136315696644\n",
            "Validation accuracy:  0.8090111642743222\n",
            "Verification accuracy  0.8351222420989863\n",
            "Summary accuracy  0.8321407274895647\n",
            "content classification accuracy  0.8717948717948718\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1918  279]\n",
            " [ 284  873]]\n",
            "*********SUMMARIZATION********\n",
            "[[2695  105]\n",
            " [ 325  229]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 362   38   19    8]\n",
            " [  57  508   84   10]\n",
            " [  34   45  351   35]\n",
            " [  36   64  123 1580]]\n",
            "validation started.. 836\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.3487379151440802\n",
            "Training accuracy:  0.8931623931623932\n",
            "Validation loss:  0.5787031732077869\n",
            "Validation accuracy:  0.8421052631578947\n",
            "Verification accuracy  0.8780560524746571\n",
            "Summary accuracy  0.8798449612403101\n",
            "content classification accuracy  0.9215861657722123\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1994  203]\n",
            " [ 200  957]]\n",
            "*********SUMMARIZATION********\n",
            "[[2715   85]\n",
            " [ 178  376]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 385   29   10    3]\n",
            " [  39  562   52    6]\n",
            " [  18   31  392   24]\n",
            " [  22   60  115 1606]]\n",
            "validation started.. 836\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.23826605306849594\n",
            "Training accuracy:  0.9314251639833034\n",
            "Validation loss:  0.6064229146489557\n",
            "Validation accuracy:  0.8572567783094098\n",
            "Verification accuracy  0.9123434704830053\n",
            "Summary accuracy  0.9239713774597496\n",
            "content classification accuracy  0.9579606440071556\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[2054  143]\n",
            " [ 112 1045]]\n",
            "*********SUMMARIZATION********\n",
            "[[2755   45]\n",
            " [  96  458]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 390   22   12    3]\n",
            " [  22  601   31    5]\n",
            " [  20   10  414   21]\n",
            " [  20   50   78 1655]]\n",
            "validation started.. 836\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.14764604349398897\n",
            "Training accuracy:  0.958656330749354\n",
            "Validation loss:  0.79281303820745\n",
            "Validation accuracy:  0.839712918660287\n",
            "Verification accuracy  0.9493142516398331\n",
            "Summary accuracy  0.9469290399522957\n",
            "content classification accuracy  0.9797257006559332\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[2092  105]\n",
            " [  73 1084]]\n",
            "*********SUMMARIZATION********\n",
            "[[2777   23]\n",
            " [  45  509]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 402   17    4    4]\n",
            " [  12  629   13    5]\n",
            " [  11    8  437    9]\n",
            " [  14   31   42 1716]]\n",
            "Validation accuracy of the model is  0.8572567783094098\n",
            "Validation loss of the model is  0.6064229146489557\n",
            "Now Testing: germanwings-crash.txt\n",
            "===================   TESTING   =====================\n",
            "germanwings-crash.txt accuracy: 0.7121535181236673\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.84286   0.51082   0.63612       231\n",
            "           1    0.65653   0.90756   0.76190       238\n",
            "\n",
            "    accuracy                        0.71215       469\n",
            "   macro avg    0.74970   0.70919   0.69901       469\n",
            "weighted avg    0.74831   0.71215   0.69995       469\n",
            "\n",
            "confusion matrix \n",
            "[[118 113]\n",
            " [ 22 216]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.76840   0.99440   0.86691       357\n",
            "         1.0    0.71429   0.04464   0.08403       112\n",
            "\n",
            "    accuracy                        0.76759       469\n",
            "   macro avg    0.74134   0.51952   0.47547       469\n",
            "weighted avg    0.75548   0.76759   0.67996       469\n",
            "\n",
            "confusion matrix \n",
            "[[355   2]\n",
            " [107   5]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.55670   0.72000   0.62791        75\n",
            "           1    0.68966   0.13605   0.22727       147\n",
            "           2    0.35841   0.76415   0.48795       106\n",
            "           3    0.83761   0.69504   0.75969       141\n",
            "\n",
            "    accuracy                        0.53945       469\n",
            "   macro avg    0.61059   0.57881   0.52571       469\n",
            "weighted avg    0.63801   0.53945   0.51032       469\n",
            "\n",
            "confusion matrix \n",
            "[[54  2 18  1]\n",
            " [19 20 95 13]\n",
            " [16  4 81  5]\n",
            " [ 8  3 32 98]]\n",
            "Validation accuracy of the model is  0.839712918660287\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'sydneysiege.txt', 'charliehebdo.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 890\n",
            "size of training data 3018\n",
            "\n",
            "training started....\n",
            "validation started.. 751\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7103759589649382\n",
            "Training accuracy:  0.7417715926662248\n",
            "Validation loss:  0.5609629357114752\n",
            "Validation accuracy:  0.8078118064802485\n",
            "Verification accuracy  0.672962226640159\n",
            "Summary accuracy  0.7319416832339297\n",
            "content classification accuracy  0.8204108681245859\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1840  204]\n",
            " [ 605  369]]\n",
            "*********SUMMARIZATION********\n",
            "[[2386   80]\n",
            " [ 462   90]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 197   76   52   20]\n",
            " [  86  351  152   34]\n",
            " [  51   79  231   53]\n",
            " [  43  102  239 1252]]\n",
            "validation started.. 751\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.4904763619735758\n",
            "Training accuracy:  0.844599072233267\n",
            "Validation loss:  0.5502396691986855\n",
            "Validation accuracy:  0.8348868175765646\n",
            "Verification accuracy  0.8326706428098078\n",
            "Summary accuracy  0.8316766070245195\n",
            "content classification accuracy  0.8694499668654738\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1799  245]\n",
            " [ 263  711]]\n",
            "*********SUMMARIZATION********\n",
            "[[2360  106]\n",
            " [ 288  264]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 277   33   23   12]\n",
            " [  48  499   66   10]\n",
            " [  38   41  296   39]\n",
            " [  29   55  111 1441]]\n",
            "validation started.. 751\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.34357676394875086\n",
            "Training accuracy:  0.8972829688535454\n",
            "Validation loss:  0.5723395959493962\n",
            "Validation accuracy:  0.8402130492676432\n",
            "Verification accuracy  0.8883366467859509\n",
            "Summary accuracy  0.8830351225977469\n",
            "content classification accuracy  0.9204771371769384\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1862  182]\n",
            " [ 171  803]]\n",
            "*********SUMMARIZATION********\n",
            "[[2384   82]\n",
            " [ 158  394]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 307   23   14    1]\n",
            " [  31  542   43    7]\n",
            " [  21   19  350   24]\n",
            " [  23   40   91 1482]]\n",
            "validation started.. 751\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.23014381898458672\n",
            "Training accuracy:  0.9297548045062957\n",
            "Validation loss:  0.6524803280196292\n",
            "Validation accuracy:  0.823346648912561\n",
            "Verification accuracy  0.9218025182239894\n",
            "Summary accuracy  0.9145129224652088\n",
            "content classification accuracy  0.9529489728296885\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1915  129]\n",
            " [ 129  845]]\n",
            "*********SUMMARIZATION********\n",
            "[[2410   56]\n",
            " [  86  466]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 321    8   10    6]\n",
            " [  16  573   28    6]\n",
            " [  14   11  378   11]\n",
            " [  18   34   74 1510]]\n",
            "validation started.. 751\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.16168865695516899\n",
            "Training accuracy:  0.9555997349237906\n",
            "Validation loss:  0.7242233883193199\n",
            "Validation accuracy:  0.8477585441633377\n",
            "Verification accuracy  0.9469847581179589\n",
            "Summary accuracy  0.9489728296885355\n",
            "content classification accuracy  0.9708416169648774\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1960   84]\n",
            " [  70  904]]\n",
            "*********SUMMARIZATION********\n",
            "[[2433   33]\n",
            " [  55  497]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 324   11    7    3]\n",
            " [  12  588   16    7]\n",
            " [  10    8  390    6]\n",
            " [  16   18   46 1556]]\n",
            "Validation accuracy of the model is  0.8477585441633377\n",
            "Validation loss of the model is  0.7242233883193199\n",
            "Now Testing: ottawashooting.txt\n",
            "===================   TESTING   =====================\n",
            "ottawashooting.txt accuracy: 0.7191011235955056\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.64757   0.88810   0.74900       420\n",
            "           1    0.85032   0.56809   0.68112       470\n",
            "\n",
            "    accuracy                        0.71910       890\n",
            "   macro avg    0.74894   0.72809   0.71506       890\n",
            "weighted avg    0.75464   0.71910   0.71315       890\n",
            "\n",
            "confusion matrix \n",
            "[[373  47]\n",
            " [203 267]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.89272   0.86968   0.88105       775\n",
            "         1.0    0.25185   0.29565   0.27200       115\n",
            "\n",
            "    accuracy                        0.79551       890\n",
            "   macro avg    0.57228   0.58266   0.57652       890\n",
            "weighted avg    0.80991   0.79551   0.80235       890\n",
            "\n",
            "confusion matrix \n",
            "[[674 101]\n",
            " [ 81  34]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.77465   0.30387   0.43651       181\n",
            "           1    0.48319   0.58673   0.52995       196\n",
            "           2    0.50698   0.64497   0.56771       169\n",
            "           3    0.80874   0.86047   0.83380       344\n",
            "\n",
            "    accuracy                        0.64607       890\n",
            "   macro avg    0.64339   0.59901   0.59199       890\n",
            "weighted avg    0.67281   0.64607   0.63556       890\n",
            "\n",
            "confusion matrix \n",
            "[[ 55  81  26  19]\n",
            " [  9 115  47  25]\n",
            " [  4  30 109  26]\n",
            " [  3  12  33 296]]\n",
            "Validation accuracy of the model is  0.8477585441633377\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n",
            "model intialising...\n",
            "Training Set: {'ottawashooting.txt', 'charliehebdo.txt', 'germanwings-crash.txt'}\n",
            "Size of test data 1221\n",
            "size of training data 2752\n",
            "\n",
            "training started....\n",
            "validation started.. 686\n",
            "Iteration  0\n",
            "errors  0\n",
            "Training Loss:  0.7157697225379389\n",
            "Training accuracy:  0.7308624031007752\n",
            "Validation loss:  0.5650146901607513\n",
            "Validation accuracy:  0.8109815354713313\n",
            "Verification accuracy  0.6558866279069767\n",
            "Summary accuracy  0.734375\n",
            "content classification accuracy  0.8023255813953488\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1640  179]\n",
            " [ 552  381]]\n",
            "*********SUMMARIZATION********\n",
            "[[2145   74]\n",
            " [ 470   63]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 243   77   37   13]\n",
            " [ 128  292  145   12]\n",
            " [  42   93  192   46]\n",
            " [  53  132  169 1078]]\n",
            "validation started.. 686\n",
            "Iteration  1\n",
            "errors  0\n",
            "Training Loss:  0.4692043259220068\n",
            "Training accuracy:  0.8490794573643411\n",
            "Validation loss:  0.5282089488450871\n",
            "Validation accuracy:  0.8381924198250729\n",
            "Verification accuracy  0.8204941860465116\n",
            "Summary accuracy  0.8590116279069767\n",
            "content classification accuracy  0.8677325581395349\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1644  175]\n",
            " [ 213  720]]\n",
            "*********SUMMARIZATION********\n",
            "[[2099  120]\n",
            " [ 244  289]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 317   31   17    5]\n",
            " [  54  424   93    6]\n",
            " [  24   51  268   30]\n",
            " [  33   48  102 1249]]\n",
            "validation started.. 686\n",
            "Iteration  2\n",
            "errors  0\n",
            "Training Loss:  0.3310989565974058\n",
            "Training accuracy:  0.9001937984496124\n",
            "Validation loss:  0.533704431251038\n",
            "Validation accuracy:  0.8411078717201166\n",
            "Verification accuracy  0.875\n",
            "Summary accuracy  0.9058866279069767\n",
            "content classification accuracy  0.9196947674418605\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1692  127]\n",
            " [ 132  801]]\n",
            "*********SUMMARIZATION********\n",
            "[[2148   71]\n",
            " [ 150  383]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 334   22   12    2]\n",
            " [  35  477   60    5]\n",
            " [  17   28  310   18]\n",
            " [  21   42   82 1287]]\n",
            "validation started.. 686\n",
            "Iteration  3\n",
            "errors  0\n",
            "Training Loss:  0.21016601247842923\n",
            "Training accuracy:  0.9393168604651162\n",
            "Validation loss:  0.5671332052280736\n",
            "Validation accuracy:  0.8479105928085521\n",
            "Verification accuracy  0.918968023255814\n",
            "Summary accuracy  0.9411337209302325\n",
            "content classification accuracy  0.9578488372093024\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1725   94]\n",
            " [  68  865]]\n",
            "*********SUMMARIZATION********\n",
            "[[2174   45]\n",
            " [  71  462]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 349   12    7    2]\n",
            " [  18  517   35    7]\n",
            " [  13   14  336   10]\n",
            " [  17   33   55 1327]]\n",
            "validation started.. 686\n",
            "Iteration  4\n",
            "errors  0\n",
            "Training Loss:  0.14130259645279758\n",
            "Training accuracy:  0.9596656976744186\n",
            "Validation loss:  0.6245048493146896\n",
            "Validation accuracy:  0.8517978620019436\n",
            "Verification accuracy  0.9425872093023255\n",
            "Summary accuracy  0.9625726744186046\n",
            "content classification accuracy  0.9738372093023255\n",
            "Training confusion matrix: \n",
            "*********VERIFICATION********\n",
            "[[1760   59]\n",
            " [  44  889]]\n",
            "*********SUMMARIZATION********\n",
            "[[2193   26]\n",
            " [  46  487]]\n",
            "*********CONTENT-CLASSIFICATION********\n",
            "[[ 350   10    7    3]\n",
            " [  15  543   18    1]\n",
            " [  10    8  348    7]\n",
            " [  13   27   39 1353]]\n",
            "Validation accuracy of the model is  0.8517978620019436\n",
            "Validation loss of the model is  0.6245048493146896\n",
            "Now Testing: sydneysiege.txt\n",
            "===================   TESTING   =====================\n",
            "sydneysiege.txt accuracy: 0.7182637182637183\n",
            "*****VERIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.71156   0.85408   0.77633       699\n",
            "           1    0.73298   0.53640   0.61947       522\n",
            "\n",
            "    accuracy                        0.71826      1221\n",
            "   macro avg    0.72227   0.69524   0.69790      1221\n",
            "weighted avg    0.72072   0.71826   0.70927      1221\n",
            "\n",
            "confusion matrix \n",
            "[[597 102]\n",
            " [242 280]]\n",
            "*****SUMMARIZATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0    0.91182   0.84026   0.87458      1083\n",
            "         1.0    0.22422   0.36232   0.27701       138\n",
            "\n",
            "    accuracy                        0.78624      1221\n",
            "   macro avg    0.56802   0.60129   0.57579      1221\n",
            "weighted avg    0.83411   0.78624   0.80704      1221\n",
            "\n",
            "confusion matrix \n",
            "[[910 173]\n",
            " [ 88  50]]\n",
            "*****CONTENT-CLASSIFICATION*****\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.47619   0.20270   0.28436       148\n",
            "           1    0.44390   0.76151   0.56086       239\n",
            "           2    0.55233   0.42411   0.47980       224\n",
            "           3    0.90625   0.85574   0.88027       610\n",
            "\n",
            "    accuracy                        0.67895      1221\n",
            "   macro avg    0.59467   0.56101   0.55132      1221\n",
            "weighted avg    0.69869   0.67895   0.67205      1221\n",
            "\n",
            "confusion matrix \n",
            "[[ 30 107   2   9]\n",
            " [  4 182  34  19]\n",
            " [ 25  78  95  26]\n",
            " [  4  43  41 522]]\n",
            "Validation accuracy of the model is  0.8517978620019436\n",
            "Validation loss of the model is  1\n",
            "Training and Testing Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41A3x9r85l7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}